{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margot-bonilla/violent-behaviour-recognition/blob/master/notebooks/TFM_2024_Iteration_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac31ede0095c5e87",
      "metadata": {
        "collapsed": false,
        "id": "ac31ede0095c5e87"
      },
      "source": [
        "# Violence Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "zwTPKaGmmtkI"
      },
      "id": "zwTPKaGmmtkI"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "oS-8e9IgbtsD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS-8e9IgbtsD",
        "outputId": "d8319883-62ff-47d1-9ad2-fd96da40e309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Crate necessary folders for the execution\n",
        "info_path = '/content/Info'\n",
        "if not os.path.exists(info_path):\n",
        "    os.makedirs(info_path)\n",
        "\n",
        "logs_path = '/content/logs'\n",
        "if not os.path.exists(logs_path):\n",
        "    os.makedirs(logs_path)\n",
        "\n",
        "models_path = '/content/models/checkpoints'\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)"
      ],
      "metadata": {
        "id": "D9yzQix5kT01"
      },
      "id": "D9yzQix5kT01",
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6a498eb73dee0160",
      "metadata": {
        "collapsed": false,
        "id": "6a498eb73dee0160"
      },
      "source": [
        "## Constant information to set the data and engineering resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "486bf0f28b9ea3a4",
      "metadata": {
        "id": "486bf0f28b9ea3a4"
      },
      "outputs": [],
      "source": [
        "# Path information to load videos and annotations\n",
        "import os\n",
        "\n",
        "ROOT_PATH = r\"/content/drive/MyDrive/UOC/TFM_2\"\n",
        "ANNOTATIONS_PATH = os.path.join(ROOT_PATH, \"Annotations\")\n",
        "VIDEOS_PATH = os.path.join(ROOT_PATH, \"Videos\")\n",
        "INFO_PATH = \"Info\"\n",
        "MODELS_PATH = 'models'\n",
        "\n",
        "LOGS_PATH = \"logs\"\n",
        "\n",
        "DATASET_TRAIN_PATH = os.path.join(INFO_PATH, \"violence_detection_train.hdf5\")\n",
        "DATASET_VAL_PATH = os.path.join(INFO_PATH, \"violence_detection_val.hdf5\")\n",
        "DATASET_TEST_PATH = os.path.join(INFO_PATH, \"violence_detection_test.hdf5\")\n",
        "\n",
        "## Krakov\n",
        "KRAKOV_VIDEOS_PATH = os.path.join(ROOT_PATH, \"Videos\")\n",
        "INCLUDE_KRAKOV = True\n",
        "\n",
        "## Hockey\n",
        "HOCKEY_VIDEOS_PATH = os.path.join(ROOT_PATH, \"Hockey\", \"data\")\n",
        "INCLUDE_HOCKEY = True\n",
        "\n",
        "## Movies\n",
        "MOVIES_VIDEOS_PATH = os.path.join(ROOT_PATH, \"movies\")\n",
        "INCLUDE_MOVIES = True\n",
        "\n",
        "## RWF 2000\n",
        "RWF2000_VIDEOS_PATH = os.path.join(ROOT_PATH, \"2 - data\")\n",
        "INCLUDE_RWF2000 = True\n",
        "\n",
        "## RWF 2000\n",
        "VIOLENT_VIDEOS_PATH = os.path.join(ROOT_PATH, \"violentFlows\", \"ForTal\")\n",
        "INCLUDE_VIOLENT = True\n",
        "\n",
        "# Set the cells/process you want to run\n",
        "CREATE_DATASET = True\n",
        "PLOT_FRAME_DISTRIBUTION = False\n",
        "IS_TEST_RUN = False\n",
        "TEST_SIZE = 10\n",
        "INSTALL_STUFF = False\n",
        "LOAD_MODEL = False\n",
        "\n",
        "EPOCHS = 10 if IS_TEST_RUN else 70\n",
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "IMG_CHANNELS = 3 # 1 for grayscale 3 for RBG\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "MAX_NUM_FRAMES = 40\n",
        "APPLY_DATA_AUGMENTATION = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "8EzhGB2g0ITu",
      "metadata": {
        "id": "8EzhGB2g0ITu"
      },
      "outputs": [],
      "source": [
        "# global variables\n",
        "# total_videos will contain the full path of each of the videos\n",
        "total_normal_videos = list()\n",
        "total_violent_videos = list()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dedup videos\n",
        "all_video_names = set()"
      ],
      "metadata": {
        "id": "EtQ8PfPidYsp"
      },
      "id": "EtQ8PfPidYsp",
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "XA3O0PEo1NCl",
      "metadata": {
        "id": "XA3O0PEo1NCl"
      },
      "outputs": [],
      "source": [
        "def include_videos(video_path, normal_prefix):\n",
        "  for v in os.listdir(video_path):\n",
        "    total_path = os.path.join(video_path, v)\n",
        "    if v.startswith(normal_prefix) and v not in all_video_names:\n",
        "      total_normal_videos.append(total_path)\n",
        "      all_video_names.add(v)\n",
        "    elif v not in all_video_names:\n",
        "      total_violent_videos.append(total_path)\n",
        "      all_video_names.add(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "QTbjRNRP0QyK",
      "metadata": {
        "id": "QTbjRNRP0QyK"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_KRAKOV:\n",
        "  include_videos(os.path.join(KRAKOV_VIDEOS_PATH), 'Normal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "SPHhjSmU1GrU",
      "metadata": {
        "id": "SPHhjSmU1GrU"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_HOCKEY:\n",
        "  include_videos(os.path.join(HOCKEY_VIDEOS_PATH), 'no')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "7hv6uyvQ-hQ_",
      "metadata": {
        "id": "7hv6uyvQ-hQ_"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_MOVIES:\n",
        "  for f in os.listdir(MOVIES_VIDEOS_PATH):\n",
        "    folder_path_normal = os.path.join(MOVIES_VIDEOS_PATH, f, \"NonViolence\")\n",
        "    folder_path_violent = os.path.join(MOVIES_VIDEOS_PATH, f, \"Violence\")\n",
        "    for normal in os.listdir(folder_path_normal):\n",
        "      if normal not in all_video_names:\n",
        "        total_normal_videos.append(os.path.join(folder_path_normal, normal))\n",
        "        all_video_names.add(normal)\n",
        "    for violent in os.listdir(folder_path_violent):\n",
        "      if violent not in all_video_names:\n",
        "        total_violent_videos.append(os.path.join(folder_path_violent, violent))\n",
        "        all_video_names.add(violent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "evZje8oUv10h",
      "metadata": {
        "id": "evZje8oUv10h"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_RWF2000:\n",
        "  for f in os.listdir(RWF2000_VIDEOS_PATH):\n",
        "    folder_path_normal = os.path.join(RWF2000_VIDEOS_PATH, f, \"NonFight\")\n",
        "    folder_path_violent = os.path.join(RWF2000_VIDEOS_PATH, f, \"Fight\")\n",
        "    for normal in os.listdir(folder_path_normal):\n",
        "      if normal not in all_video_names:\n",
        "        total_normal_videos.append(os.path.join(folder_path_normal, normal))\n",
        "        all_video_names.add(normal)\n",
        "    for violent in os.listdir(folder_path_violent):\n",
        "      if violent not in all_video_names:\n",
        "        total_violent_videos.append(os.path.join(folder_path_violent, violent))\n",
        "        all_video_names.add(violent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "RpLN9NS7O2JI",
      "metadata": {
        "id": "RpLN9NS7O2JI"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_VIOLENT:\n",
        "  for v in os.listdir(VIOLENT_VIDEOS_PATH):\n",
        "    if v not in all_video_names:\n",
        "      total_violent_videos.append(os.path.join(VIOLENT_VIDEOS_PATH, v))\n",
        "      all_video_names.add(v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dedup videos\n",
        "print(\"all video names: \", len(all_video_names))\n",
        "print(\"all dedup video names: \", len(total_normal_videos) + len(total_violent_videos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzbHj6cpdR1n",
        "outputId": "89480a63-fb14-4d52-e6f7-1ff2d430098b"
      },
      "id": "TzbHj6cpdR1n",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all video names:  6678\n",
            "all dedup video names:  6678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "TsRpLMiOwnNA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsRpLMiOwnNA",
        "outputId": "8b8aab27-645f-4fa1-b1ad-81b9da44a388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of normal videos included: 4623\n",
            "Total of violent videos included: 2055\n"
          ]
        }
      ],
      "source": [
        "print(f'Total of normal videos included: {len(total_normal_videos)}')\n",
        "print(f'Total of violent videos included: {len(total_violent_videos)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries that you may need to run the notebook"
      ],
      "metadata": {
        "id": "z8Vr9PmBh-bC"
      },
      "id": "z8Vr9PmBh-bC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this if you are not able to install tensorflow properly\n",
        "if INSTALL_STUFF:\n",
        "    !pip install tensorflow[and-cuda]\n",
        "    !pip install pydot\n",
        "    !pip install graphviz"
      ],
      "metadata": {
        "id": "j21fk1tsh7DD"
      },
      "id": "j21fk1tsh7DD",
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "EQDddyYZz1hj",
      "metadata": {
        "id": "EQDddyYZz1hj"
      },
      "source": [
        "## Plot distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "g78SxdjzqHQb",
      "metadata": {
        "id": "g78SxdjzqHQb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy import stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import progressbar\n",
        "\n",
        "def create_frame_distribution(videos):\n",
        "    number_of_frames = list()\n",
        "    print(f\"Total videos: {len(videos)}\")\n",
        "    bar = progressbar.ProgressBar(maxval=len(videos), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    bar.start()\n",
        "    processed = 0\n",
        "    for video in videos:\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video)\n",
        "            property_id = int(cv2.CAP_PROP_FRAME_COUNT)\n",
        "            length = int(cv2.VideoCapture.get(cap, property_id))\n",
        "            number_of_frames.append(length)\n",
        "            cap.release()\n",
        "\n",
        "            processed += 1\n",
        "            bar.update(processed)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {e.message}\")\n",
        "    bar.finish()\n",
        "\n",
        "    return number_of_frames\n",
        "\n",
        "def plot_frame_distribution(number_of_frames):\n",
        "    frame_counts = np.array(number_of_frames)\n",
        "    print(\"Number of unique frame counts:\", len(frame_counts))\n",
        "    print(\"Max frames in a video:\", frame_counts.max())\n",
        "    print(\"Min frames in a video:\", frame_counts.min())\n",
        "    print(\"Average frames per video:\", frame_counts.mean())\n",
        "    print(\"Median frames per video:\", np.median(frame_counts))\n",
        "    mode_value = st.mode(frame_counts).mode\n",
        "    print(\"Mode frames per video:\", mode_value)\n",
        "    print(\"Videos over 2k frames:\", np.sum(frame_counts > 2000))\n",
        "    print(\"Videos over 5k frames:\", np.sum(frame_counts > 5000))\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(frame_counts, bins=100, color='skyblue', edgecolor='black')\n",
        "\n",
        "    # Add vertical lines for mean, median, and mode\n",
        "    plt.axvline(frame_counts.mean(), color='red', linestyle='dashed', linewidth=1, label='Mean')\n",
        "    plt.axvline(np.median(frame_counts), color='yellow', linestyle='dashed', linewidth=1, label='Median')\n",
        "    plt.axvline(mode_value, color='green', linestyle='dashed', linewidth=1, label='Mode')\n",
        "\n",
        "    # Highlight majority population range (for example, mean ± one standard deviation)\n",
        "    lower_bound = frame_counts.mean() - frame_counts.std()\n",
        "    upper_bound = frame_counts.mean() + frame_counts.std()\n",
        "    plt.axvspan(lower_bound, upper_bound, color='orange', alpha=0.2, label='Majority Range')\n",
        "\n",
        "    plt.title('Distribution of Number of Frames per Video')\n",
        "    plt.xlabel('Number of Frames')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "LqW_s8Kfz6U5",
      "metadata": {
        "id": "LqW_s8Kfz6U5"
      },
      "outputs": [],
      "source": [
        "if PLOT_FRAME_DISTRIBUTION:\n",
        "  number_of_frames = create_frame_distribution(total_normal_videos + total_violent_videos)\n",
        "  plot_frame_distribution(number_of_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fD81d8w8foby",
      "metadata": {
        "id": "fD81d8w8foby"
      },
      "source": [
        "# Process video data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "ae01b7e638c07ebd",
      "metadata": {
        "id": "ae01b7e638c07ebd"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "# class syntax\n",
        "class Label(Enum):\n",
        "    VIOLENT = \"Violent\"\n",
        "    NORMAL = \"Normal\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "c49ad848fd4d8445",
      "metadata": {
        "id": "c49ad848fd4d8445"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import h5py\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "def duplicate_frames(frames, target_count):\n",
        "  # Duplicate the last available frame to fill missing frames\n",
        "  last_frame = frames[-1]\n",
        "  while len(frames) < target_count:\n",
        "      frames.append(last_frame.copy())\n",
        "  return frames\n",
        "\n",
        "def interpolate_frames(frames, keypoints, max_frames):\n",
        "  current_length = len(frames)\n",
        "  if current_length == max_frames:\n",
        "    return frames, keypoints\n",
        "\n",
        "  frame_indices = np.arange(current_length)\n",
        "  target_indices = np.linspace(0, current_length - 1, max_frames)\n",
        "\n",
        "  interpolated_frames = []\n",
        "  # Assuming frames of shape (H, W, C)\n",
        "  for i in range(frames[0].shape[2]):\n",
        "    f = interp1d(frame_indices, [frame[:,:,i] for frame in frames], axis=0, kind='linear')\n",
        "    interpolated_channel = f(target_indices)\n",
        "    if i == 0:\n",
        "      interpolated_frames = np.zeros((max_frames, interpolated_channel.shape[1], interpolated_channel.shape[2], frames[0].shape[2]))\n",
        "    interpolated_frames[:,:,:,i] = interpolated_channel\n",
        "\n",
        "  interpolated_keypoints = []\n",
        "  # Assuming 33 keypoints\n",
        "  for i in range(len(keypoints[0])):\n",
        "    f = interp1d(frame_indices, [keypoint[i] for keypoint in keypoints], axis=0, kind='linear')\n",
        "    interpolated_keypoint = f(target_indices)\n",
        "    interpolated_keypoints.append(interpolated_keypoint)\n",
        "\n",
        "  interpolated_keypoints = np.array(interpolated_keypoints).transpose((1, 0, 2))\n",
        "\n",
        "  return interpolated_frames, interpolated_keypoints\n",
        "\n",
        "\n",
        "def normalize(frame):\n",
        "  # Normalize the frame\n",
        "  frame = frame.astype(np.float32) / 255.0\n",
        "\n",
        "  return frame\n",
        "\n",
        "def resize(frame, target_size=(IMG_WIDTH, IMG_HEIGHT)):\n",
        "  # Resize all frames to same dimension\n",
        "  frame = cv2.resize(frame, dsize=target_size)\n",
        "\n",
        "  return frame\n",
        "\n",
        "def convert_grayscale(frame):\n",
        "  # Convert to grayscale\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  return frame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if INSTALL_STUFF:\n",
        "  !pip install mediapipe"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lTYcn47Dxy9u"
      },
      "id": "lTYcn47Dxy9u",
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "def extract_pose_keypoints(frame):\n",
        "  \"\"\"\n",
        "  Will add only body and basic poses\n",
        "  \"\"\"\n",
        "  mp_pose = mp.solutions.pose\n",
        "  pose = mp_pose.Pose()\n",
        "\n",
        "  frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  results = pose.process(frame_rgb)\n",
        "  if results.pose_landmarks:\n",
        "    person_landmarks = results.pose_landmarks\n",
        "    # keypoints = [(lm.x, lm.y, lm.z) for lm in results.pose_landmarks.landmark]\n",
        "    keypoints = np.zeros((13, 3), dtype=np.float32)\n",
        "\n",
        "    # Extract body keypoints based on their index\n",
        "    keypoints[0] = (person_landmarks.landmark[1].x, person_landmarks.landmark[1].y, person_landmarks.landmark[1].z)  # Left shoulder\n",
        "    keypoints[1] = (person_landmarks.landmark[2].x, person_landmarks.landmark[2].y, person_landmarks.landmark[2].z)  # Right shoulder\n",
        "    keypoints[2] = (person_landmarks.landmark[3].x, person_landmarks.landmark[3].y, person_landmarks.landmark[3].z)  # Left elbow\n",
        "    keypoints[3] = (person_landmarks.landmark[4].x, person_landmarks.landmark[4].y, person_landmarks.landmark[4].z)  # Right elbow\n",
        "    keypoints[4] = (person_landmarks.landmark[5].x, person_landmarks.landmark[5].y, person_landmarks.landmark[5].z)  # Left wrist\n",
        "    keypoints[5] = (person_landmarks.landmark[6].x, person_landmarks.landmark[6].y, person_landmarks.landmark[6].z)  # Right wrist\n",
        "    keypoints[6] = (person_landmarks.landmark[8].x, person_landmarks.landmark[8].y, person_landmarks.landmark[8].z)  # left hip\n",
        "    keypoints[7] = (person_landmarks.landmark[9].x, person_landmarks.landmark[9].y, person_landmarks.landmark[9].z)  # right hip\n",
        "    keypoints[8] = (person_landmarks.landmark[10].x, person_landmarks.landmark[10].y, person_landmarks.landmark[10].z)  # Left knee\n",
        "    keypoints[9] = (person_landmarks.landmark[11].x, person_landmarks.landmark[11].y, person_landmarks.landmark[11].z)  # Right Knee\n",
        "    keypoints[10] = (person_landmarks.landmark[12].x, person_landmarks.landmark[12].y, person_landmarks.landmark[12].z)  # Left Angle\n",
        "    keypoints[11] = (person_landmarks.landmark[13].x, person_landmarks.landmark[13].y, person_landmarks.landmark[13].z)  # Right Angle\n",
        "    keypoints[12] = (person_landmarks.landmark[15].x, person_landmarks.landmark[15].y, person_landmarks.landmark[15].z)  # Neck\n",
        "  else:\n",
        "    # Assuming 33 keypoints for consistency\n",
        "    keypoints = [(0, 0, 0)] * 13\n",
        "\n",
        "  pose.close()\n",
        "\n",
        "  return keypoints"
      ],
      "metadata": {
        "id": "buMG0owh0NFK"
      },
      "id": "buMG0owh0NFK",
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_hdf5(hdf5_file, video_name, frames, keypoints, label):\n",
        "  with h5py.File(hdf5_file, 'a') as f:\n",
        "    # Check if the group for the video already exists\n",
        "    if video_name in f:\n",
        "      # Delete existing datasets within the group (if any)\n",
        "      del f[f'{video_name}']\n",
        "    # Create a new group for the video data\n",
        "    group = f.create_group(video_name)\n",
        "\n",
        "    # Save frames, keypoints, and label as datasets within the group\n",
        "    group.create_dataset('frames', data=np.array(frames))\n",
        "    group.create_dataset('keypoints', data=np.array(keypoints))\n",
        "    group.create_dataset('label', data=np.array([label]), dtype='i')\n"
      ],
      "metadata": {
        "id": "b6Bx4wOQBToY"
      },
      "id": "b6Bx4wOQBToY",
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "id": "680a726710172130",
      "metadata": {
        "id": "680a726710172130"
      },
      "outputs": [],
      "source": [
        "# Function to process a single video and its annotation\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def process_video(video_path, hdf5_file, video_name, label, max_frames):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  frames = []\n",
        "  keypoints_list = []\n",
        "\n",
        "  while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    frame_resized = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    frame_normalized = frame_resized / 255.0\n",
        "    frames.append(frame_normalized)\n",
        "\n",
        "    keypoints = extract_pose_keypoints(frame)\n",
        "    keypoints_list.append(keypoints)\n",
        "\n",
        "  cap.release()\n",
        "\n",
        "  # Ensure consistency frame length (padding or truncating)\n",
        "  num_frames = len(frames)\n",
        "  if num_frames < max_frames:\n",
        "    frames, keypoints_list = interpolate_frames(frames, keypoints_list, max_frames)\n",
        "  elif num_frames > max_frames:\n",
        "    segment_count = num_frames // max_frames\n",
        "    for i in range(segment_count):\n",
        "      segment_frames = frames[i * max_frames:(i + 1) * max_frames]\n",
        "      segment_keypoints = keypoints_list[i * max_frames:(i + 1) * max_frames]\n",
        "      save_to_hdf5(hdf5_file, f'{video_name}_segment_{i}', segment_frames, segment_keypoints, label)\n",
        "\n",
        "    # Handle remaining frames if any\n",
        "    if num_frames % max_frames != 0:\n",
        "      remaining_frames = frames[segment_count * max_frames:]\n",
        "      remaining_keypoints = keypoints_list[segment_count * max_frames:]\n",
        "      frames, keypoints_list = interpolate_frames(remaining_frames, remaining_keypoints, max_frames)\n",
        "      save_to_hdf5(hdf5_file, f'{video_name}_segment_{segment_count}', frames, keypoints_list, label)\n",
        "\n",
        "    return\n",
        "  save_to_hdf5(hdf5_file, video_name, frames, keypoints_list, label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test save video with frames, keypoints, and label\n",
        "video_path = '/content/drive/MyDrive/UOC/TFM_2/Hockey/data/fi107_xvid.avi'\n",
        "hdf5_file = 'test.hdf5'\n",
        "video_name = 'fi107_xvid'\n",
        "label = 1\n",
        "max_frames = 20\n",
        "process_video(video_path, hdf5_file, video_name, label, max_frames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daszAff5B1uL",
        "outputId": "609eb0c8-6307-4dc2-e3f7-145b1b4916e2"
      },
      "id": "daszAff5B1uL",
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_interpolate.py:701: RuntimeWarning: invalid value encountered in divide\n",
            "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "def explore_hdf5_file(hdf5_file_path):\n",
        "    def print_attrs(name, obj):\n",
        "        print(f\"{name}: {obj}\")\n",
        "        if isinstance(obj, h5py.Dataset):\n",
        "            print(f\"  Shape: {obj.shape}\")\n",
        "            print(f\"  Data type: {obj.dtype}\")\n",
        "        elif isinstance(obj, h5py.Group):\n",
        "            print(f\"  Contains: {list(obj.keys())}\")\n",
        "\n",
        "    with h5py.File(hdf5_file_path, 'r') as f:\n",
        "        f.visititems(print_attrs)\n",
        "\n",
        "# Example usage\n",
        "print(\"Test file contents:\")\n",
        "explore_hdf5_file(hdf5_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q1qNND7rRX9o",
        "outputId": "833ef0fd-168d-4e34-efb2-817ad5a16347"
      },
      "id": "q1qNND7rRX9o",
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test file contents:\n",
            "fi107_xvid_segment_0: <HDF5 group \"/fi107_xvid_segment_0\" (3 members)>\n",
            "  Contains: ['frames', 'keypoints', 'label']\n",
            "fi107_xvid_segment_0/frames: <HDF5 dataset \"frames\": shape (20, 128, 128, 3), type \"<f8\">\n",
            "  Shape: (20, 128, 128, 3)\n",
            "  Data type: float64\n",
            "fi107_xvid_segment_0/keypoints: <HDF5 dataset \"keypoints\": shape (20, 13, 3), type \"<f8\">\n",
            "  Shape: (20, 13, 3)\n",
            "  Data type: float64\n",
            "fi107_xvid_segment_0/label: <HDF5 dataset \"label\": shape (1,), type \"<i4\">\n",
            "  Shape: (1,)\n",
            "  Data type: int32\n",
            "fi107_xvid_segment_1: <HDF5 group \"/fi107_xvid_segment_1\" (3 members)>\n",
            "  Contains: ['frames', 'keypoints', 'label']\n",
            "fi107_xvid_segment_1/frames: <HDF5 dataset \"frames\": shape (20, 128, 128, 3), type \"<f8\">\n",
            "  Shape: (20, 128, 128, 3)\n",
            "  Data type: float64\n",
            "fi107_xvid_segment_1/keypoints: <HDF5 dataset \"keypoints\": shape (20, 13, 3), type \"<f8\">\n",
            "  Shape: (20, 13, 3)\n",
            "  Data type: float64\n",
            "fi107_xvid_segment_1/label: <HDF5 dataset \"label\": shape (1,), type \"<i4\">\n",
            "  Shape: (1,)\n",
            "  Data type: int32\n",
            "fi107_xvid_segment_2: <HDF5 group \"/fi107_xvid_segment_2\" (3 members)>\n",
            "  Contains: ['frames', 'keypoints', 'label']\n",
            "fi107_xvid_segment_2/frames: <HDF5 dataset \"frames\": shape (20, 128, 128, 3), type \"<f8\">\n",
            "  Shape: (20, 128, 128, 3)\n",
            "  Data type: float64\n",
            "fi107_xvid_segment_2/keypoints: <HDF5 dataset \"keypoints\": shape (20, 13, 3), type \"<f8\">\n",
            "  Shape: (20, 13, 3)\n",
            "  Data type: float64\n",
            "fi107_xvid_segment_2/label: <HDF5 dataset \"label\": shape (1,), type \"<i4\">\n",
            "  Shape: (1,)\n",
            "  Data type: int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "hRB9y2rJqO_B",
      "metadata": {
        "id": "hRB9y2rJqO_B"
      },
      "outputs": [],
      "source": [
        "import progressbar\n",
        "\n",
        "\n",
        "def process_and_save_videos(videos, train_file, val_file):\n",
        "  \"\"\"\n",
        "  viceos: Tuple: (video_path: string, label: Label)\n",
        "  train_file: hdf5 file for train dataset\n",
        "  val_file: hdf5 file for val dataset\n",
        "  \"\"\"\n",
        "  bar = progressbar.ProgressBar(maxval=len(videos), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "  bar.start()\n",
        "  for i, (video_path, label) in enumerate(videos):\n",
        "    video_name = video_path.split('/')[-1].split('.')[0]\n",
        "    if i / len(videos) < 0.8:\n",
        "      process_video(video_path, train_file, video_name, label, MAX_NUM_FRAMES)\n",
        "    else:\n",
        "      process_video(video_path, val_file, video_name, label, MAX_NUM_FRAMES)\n",
        "\n",
        "    bar.update(i)\n",
        "  bar.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "ff44a42cbd0f4b72",
      "metadata": {
        "id": "ff44a42cbd0f4b72"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def save_dataset(all_videos):\n",
        "    try:\n",
        "        # load less volume for testing purposes\n",
        "        if IS_TEST_RUN:\n",
        "            all_videos = random.sample(all_videos, TEST_SIZE)\n",
        "\n",
        "        # Process normal videos\n",
        "        process_and_save_videos(all_videos, DATASET_TRAIN_PATH, DATASET_VAL_PATH)\n",
        "\n",
        "        total = len(all_videos)\n",
        "        print(f'\\nTotal videos processed: {total}, Train: {total * 0.8}, Val: {total * 0.2}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'ERROR: {e}')\n",
        "        print('Datasets not saved properly')\n",
        "        raise Exception(e)\n",
        "\n",
        "\n",
        "    print(\"Datasets are saved in HDF5 files correctly\")\n",
        "    print(f\"Train dataset: {DATASET_TRAIN_PATH}\")\n",
        "    print(f\"Val dataset: {DATASET_VAL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wYC--4WQi_Wh",
      "metadata": {
        "id": "wYC--4WQi_Wh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2dfa39-6994-4378-b992-fe8e46381f8f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[================                                                        ]  22%"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "# label all normal and violent videos and shuffle them\n",
        "all_videos = list()\n",
        "for normal in total_normal_videos:\n",
        "  all_videos.append((normal, 0))\n",
        "for violent in total_violent_videos:\n",
        "  all_videos.append((violent, 1))\n",
        "\n",
        "random.shuffle(all_videos)\n",
        "\n",
        "save_dataset(all_videos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606e86302bb90fa5",
      "metadata": {
        "collapsed": true,
        "id": "606e86302bb90fa5"
      },
      "outputs": [],
      "source": [
        "# Function to explore the datasets\n",
        "def traverse_datasets(hdf_file):\n",
        "    \"\"\"Traverse all datasets across all groups in HDF5 file.\"\"\"\n",
        "\n",
        "    import h5py\n",
        "\n",
        "    def h5py_dataset_iterator(g, prefix=''):\n",
        "        for key in g.keys():\n",
        "            item = g[key]\n",
        "            path = '{}/{}'.format(prefix, key)\n",
        "            if isinstance(item, h5py.Dataset):  # test for dataset\n",
        "                yield (path, item)\n",
        "            elif isinstance(item, h5py.Group):  # test for group (go down)\n",
        "                yield from h5py_dataset_iterator(item, path)\n",
        "\n",
        "    with h5py.File(hdf_file, 'r') as f:\n",
        "        print(f'Number of keys {len(f.keys())}\\n')\n",
        "        for (path, dset) in h5py_dataset_iterator(f):\n",
        "            print(path, dset)\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46aaa734678e0ef8",
      "metadata": {
        "collapsed": false,
        "id": "46aaa734678e0ef8"
      },
      "source": [
        "# Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yZYJt1Io6kCw",
      "metadata": {
        "id": "yZYJt1Io6kCw"
      },
      "source": [
        "## Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9-Z-BCoW6i-O",
      "metadata": {
        "id": "9-Z-BCoW6i-O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_keypoints(frame, keypoints):\n",
        "    plt.imshow(frame)\n",
        "    for keypoint in keypoints:\n",
        "        x, y, z = keypoint\n",
        "        plt.scatter(x * frame.shape[1], y * frame.shape[0], c='r', s=10)\n",
        "    plt.title('Frame with Keypoints')\n",
        "    plt.show()\n",
        "\n",
        "def check_generator_output(generator, batch_index=0):\n",
        "    # Get the batch from the generator\n",
        "    (frames_batch, keypoints_batch), labels_batch = generator[batch_index]\n",
        "\n",
        "    print(f\"Batch shape (frames): {frames_batch.shape}\")\n",
        "    print(f\"Batch shape (keypoints): {keypoints_batch.shape}\")\n",
        "    print(f\"Batch shape (labels): {labels_batch.shape}\")\n",
        "\n",
        "    # Print out the first frame and corresponding keypoints from the first sample in the batch\n",
        "    first_frame = frames_batch[0][0]\n",
        "    first_keypoints = keypoints_batch[0][0]\n",
        "\n",
        "    # Plot the first frame\n",
        "    plt.imshow(first_frame)\n",
        "    for x, y, _ in first_keypoints:\n",
        "        plt.scatter(x, y, c='red')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot keypoints on the first frame of the first sample\n",
        "    plot_keypoints(first_frame, first_keypoints)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Byb9qOHw6sds",
      "metadata": {
        "id": "Byb9qOHw6sds"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_frames(generator, num_batches=1, num_frames=5):\n",
        "    for i in range(num_batches):\n",
        "        X, y = generator[i]\n",
        "        for j in range(num_frames):\n",
        "            plt.imshow(X[j][0, :, :, 0])\n",
        "            plt.title(f\"Label: {y[j]}\")\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UTIbkRn56sTV",
      "metadata": {
        "id": "UTIbkRn56sTV"
      },
      "outputs": [],
      "source": [
        "def check_class_distribution(generator):\n",
        "    all_labels = []\n",
        "    for i in range(len(generator)):\n",
        "        _, y = generator[i]\n",
        "        all_labels.extend(y)\n",
        "\n",
        "    unique, counts = np.unique(all_labels, return_counts=True)\n",
        "    label_distribution = dict(zip(unique, counts))\n",
        "\n",
        "    print(\"Class Distribution:\")\n",
        "    for label, count in label_distribution.items():\n",
        "        print(f\" - Label {label}: {count} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8972781e88090f6e",
      "metadata": {
        "id": "8972781e88090f6e",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class HDF5DataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Custom data generator for HDF5 data with video fragments and labels per video.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_file, batch_size, target_size, max_frames, augmentation=None):\n",
        "        self.hdf5_file = hdf5_file\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.max_frames = max_frames\n",
        "        self.augmentation = augmentation\n",
        "        self.indices = self._get_indices()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _get_indices(self):\n",
        "        # Load data information from the HDF5 file\n",
        "        with h5py.File(self.hdf5_file, \"r\") as f:\n",
        "            return list(f.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches per epoch\n",
        "        return int(np.floor(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate a single batch of data and labels\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Load data and labels for the current batch of videos\n",
        "        X, X_keypoints, y = self.__data_generation(batch_indices)\n",
        "\n",
        "        return [X, X_keypoints], y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle indices for each epoch\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, batch_indices):\n",
        "        X = np.zeros((self.batch_size, self.max_frames, *self.target_size, 3), dtype=np.float32)\n",
        "        X_keypoints = np.zeros((self.batch_size, self.max_frames, 13, 3), dtype=np.float32)\n",
        "        y = np.zeros((self.batch_size,), dtype=np.float32)\n",
        "\n",
        "        with h5py.File(self.hdf5_file, 'r') as f:\n",
        "            for i, idx in enumerate(batch_indices):\n",
        "                frames = f[f'{idx}/frames'][:]\n",
        "                keypoints = f[f'{idx}/keypoints'][:]\n",
        "                label = f[f'{idx}/label'][()]\n",
        "\n",
        "                # Pad or truncate frames to max_frames\n",
        "                if frames.shape[0] < self.max_frames:\n",
        "                    pad_width = self.max_frames - frames.shape[0]\n",
        "                    frames = np.pad(frames, ((0, pad_width), (0, 0), (0, 0), (0, 0)), mode='constant')\n",
        "                    keypoints = np.pad(keypoints, ((0, pad_width), (0, 0), (0, 0)), mode='constant')\n",
        "                elif frames.shape[0] > self.max_frames:\n",
        "                    frames = frames[:self.max_frames]\n",
        "                    keypoints = keypoints[:self.max_frames]\n",
        "\n",
        "                if self.augmentation:\n",
        "                    augmented_frames = []\n",
        "                    for frame in frames:\n",
        "                        augmented_frame = self.augmentation.random_transform(frame)\n",
        "                        augmented_frames.append(augmented_frame)\n",
        "                    frames = np.array(augmented_frames)\n",
        "\n",
        "                X[i] = frames\n",
        "                X_keypoints[i] = keypoints\n",
        "                y[i] = label\n",
        "\n",
        "        return X, X_keypoints, y\n",
        "\n",
        "# Example usage\n",
        "max_frames = 50  # Example value, adjust as needed\n",
        "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 128, 128, 3  # Example values, adjust as needed\n",
        "\n",
        "# Create data generators\n",
        "train_generator = HDF5DataGenerator(DATASET_TRAIN_PATH, batch_size=8, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=max_frames)\n",
        "val_generator = HDF5DataGenerator(DATASET_VAL_PATH, batch_size=8, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=max_frames)\n",
        "\n",
        "# Example function to check the generator output\n",
        "def check_generator_output(generator, batch_index=0):\n",
        "    (frames_batch, keypoints_batch), labels_batch = generator[batch_index]\n",
        "\n",
        "    print(f\"Batch shape (frames): {frames_batch.shape}\")\n",
        "    print(f\"Batch shape (keypoints): {keypoints_batch.shape}\")\n",
        "    print(f\"Batch shape (labels): {labels_batch.shape}\")\n",
        "\n",
        "    # Optionally visualize the first frame and keypoints\n",
        "    first_frame = frames_batch[0][0]\n",
        "    first_keypoints = keypoints_batch[0][0]\n",
        "\n",
        "    plt.imshow(first_frame.astype(np.uint8))\n",
        "    for x, y, _ in first_keypoints:\n",
        "        plt.scatter(x, y, c='red')\n",
        "    plt.show()\n",
        "\n",
        "# Check the generator output\n",
        "check_generator_output(train_generator, batch_index=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b4233be971aeae",
      "metadata": {
        "collapsed": false,
        "id": "c0b4233be971aeae"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf28ba1094c3035",
      "metadata": {
        "id": "2bf28ba1094c3035"
      },
      "outputs": [],
      "source": [
        "def load_dataset_data(dataset_name):\n",
        "    X = []\n",
        "    y = []\n",
        "    frame_lengths = []\n",
        "    with h5py.File(dataset_name, \"r\") as hdf5_file:\n",
        "        video_names = list(hdf5_file.keys())\n",
        "        for video_name in video_names:\n",
        "            # Access frames and labels datasets for the specific video\n",
        "            frames = hdf5_file[f\"{video_name}/frames\"][:]\n",
        "            labels = hdf5_file[f\"{video_name}/labels\"][:]\n",
        "\n",
        "            # Append data and label to batch lists\n",
        "            X.append(frames)\n",
        "            y.append(labels)\n",
        "            frame_lengths.append(frames.shape[0])\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(frame_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_size(dataset_name):\n",
        "  size = 0\n",
        "  with h5py.File(dataset_name, 'r') as f:\n",
        "    video_names = list(f.keys())\n",
        "    for v in video_names:\n",
        "      size += len(f[f'{v}/frames'])\n",
        "\n",
        "  return size\n",
        "\n",
        "get_dataset_size(DATASET_TRAIN_PATH)"
      ],
      "metadata": {
        "id": "vA1NhUnRoojU",
        "collapsed": true
      },
      "id": "vA1NhUnRoojU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790a38daaa09f230",
      "metadata": {
        "collapsed": true,
        "id": "790a38daaa09f230"
      },
      "outputs": [],
      "source": [
        "if INSTALL_STUFF:\n",
        "    !pip install tensorboard\n",
        "\n",
        "# launch this command in your terminal if you want to see the tensorboard\n",
        "# !tensorboard --logdir=C:\\Users\\margo\\OneDrive\\UOC\\projects\\thesis\\logs\n",
        "\n",
        "# Open http://localhost:6006 in your browser to access tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T1iC8THk6n7S",
      "metadata": {
        "id": "T1iC8THk6n7S"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training using generators"
      ],
      "metadata": {
        "id": "XnuwRMnTx8tD"
      },
      "id": "XnuwRMnTx8tD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4755b9863a69fef",
      "metadata": {
        "id": "e4755b9863a69fef"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "\n",
        "import os\n",
        "\n",
        "def train_model_using_generators(model, epochs=EPOCHS):\n",
        "    \"\"\"Trains a model on the provided data using data generators.\n",
        "\n",
        "    Args:\n",
        "      model: The Keras model to be trained.\n",
        "      epochs: Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "      The trained model, train history, and val generator\n",
        "    \"\"\"\n",
        "\n",
        "    # Generators\n",
        "    train_generator = HDF5DataGenerator(DATASET_TRAIN_PATH, BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=MAX_NUM_FRAMES)\n",
        "    val_generator = HDF5DataGenerator(DATASET_VAL_PATH, BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=MAX_NUM_FRAMES)\n",
        "\n",
        "    # Callbacks\n",
        "    my_callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ModelCheckpoint(filepath=os.path.join(MODELS_PATH, 'checkpoints', 'model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
        "                        save_best_only=True),\n",
        "        TensorBoard(log_dir=LOGS_PATH),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "    ]\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    train_history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_generator,\n",
        "        callbacks=my_callbacks\n",
        "    )\n",
        "\n",
        "    return model, train_history, val_generator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "4LbiaqYWhHI2"
      },
      "id": "4LbiaqYWhHI2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0854e0938839272",
      "metadata": {
        "id": "c0854e0938839272"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv3D, Conv2D, MaxPooling2D, TimeDistributed, Flatten, LSTM, Dropout, Dense, BatchNormalization, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "import os\n",
        "\n",
        "def create_video_model_deprecated(input_shape=(MAX_NUM_FRAMES, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), num_classes=1):\n",
        "    model = Sequential([\n",
        "        Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling3D((1, 2, 2)),  # Pool only in spatial dimensions\n",
        "\n",
        "        Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling3D((1, 2, 2)),  # Pool only in spatial dimensions\n",
        "\n",
        "        Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling3D((1, 2, 2)),  # Pool only in spatial dimensions\n",
        "\n",
        "        Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling3D((1, 2, 2)),  # Pool only in spatial dimensions\n",
        "\n",
        "        TimeDistributed(Flatten()),  # Flatten the output of each time step\n",
        "        LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)),  # L2 regularization\n",
        "        Dropout(0.5),  # Dropout to prevent overfitting\n",
        "\n",
        "        LSTM(32, kernel_regularizer=l2(0.01)),  # Another LSTM layer for added depth\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='sigmoid')  # Sigmoid for binary classification\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the video model\n",
        "video_model_deprecated = create_video_model_deprecated()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv3D, BatchNormalization, MaxPooling3D, TimeDistributed, Flatten, LSTM, Dropout, Dense, concatenate\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def create_video_model(input_shape_frames=(MAX_NUM_FRAMES, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
        "                       input_shape_keypoints=(MAX_NUM_FRAMES, 13, 3), num_classes=1):\n",
        "\n",
        "    # Frame input branch\n",
        "    frames_input = Input(shape=input_shape_frames, name='frames_input')\n",
        "    x = Conv3D(32, (3,3,3), activation='relu', padding='same')(frames_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling3D((1,2,2))(x)\n",
        "    x = Conv3D(64, (3,3,3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling3D((1,2,2))(x)\n",
        "    x = Conv3D(128, (3,3,3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling3D((1,2,2))(x)\n",
        "    x = Conv3D(128, (3,3,3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling3D((1,2,2))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "    x = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = LSTM(32, kernel_regularizer=l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Keypoints input branch\n",
        "    keypoints_input = Input(shape=input_shape_keypoints, name='keypoints_input')\n",
        "    y = TimeDistributed(Flatten())(keypoints_input)\n",
        "    y = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "    y = LSTM(32, kernel_regularizer=l2(0.01))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "    y = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "\n",
        "    # Combine both branches\n",
        "    combined = concatenate([x, y])\n",
        "    z = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
        "    z = Dropout(0.5)(z)\n",
        "    z = Dense(num_classes, activation='sigmoid')(z)\n",
        "\n",
        "    model = Model(inputs=[frames_input, keypoints_input], outputs=z)\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "input_shape_frames = (MAX_NUM_FRAMES, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "input_shape_keypoints = (MAX_NUM_FRAMES, 13, 3)\n",
        "video_model = create_video_model(input_shape_frames=input_shape_frames, input_shape_keypoints=input_shape_keypoints, num_classes=1)\n"
      ],
      "metadata": {
        "id": "QonHD_frYjoq"
      },
      "id": "QonHD_frYjoq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z29nangkKR5V",
      "metadata": {
        "id": "z29nangkKR5V",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, TimeDistributed, LSTM\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy import stats as st\n",
        "import progressbar\n",
        "\n",
        "m = create_video_model()\n",
        "m.summary()\n",
        "\n",
        "def plot_model_diagram(model, file_path='model_diagram.png'):\n",
        "    plot_model(model, to_file=file_path, show_shapes=True, show_layer_names=True)\n",
        "    img = plt.imread(file_path)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "plot_model_diagram(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "cO4xm9EdhTD7"
      },
      "id": "cO4xm9EdhTD7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Offline Evaluation"
      ],
      "metadata": {
        "id": "o6egcu-fhY7B"
      },
      "id": "o6egcu-fhY7B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612bc4941b179af7",
      "metadata": {
        "id": "612bc4941b179af7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Train the model\n",
        "model = create_video_model()\n",
        "if LOAD_MODEL:\n",
        "    trained_model, train_history, val_generator = load_model(MODELS_PATH, 'basic_model.keras')\n",
        "else:\n",
        "    trained_model, train_history, val_generator = train_model_using_generators(model, epochs=EPOCHS)\n",
        "\n",
        "model.save(os.path.join(MODELS_PATH, 'basic_model.keras'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('train_history.pkl', 'wb') as f:\n",
        "    pickle.dump(train_history.history, f)"
      ],
      "metadata": {
        "id": "m4YWSfiayhf6"
      },
      "id": "m4YWSfiayhf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FT3poEHR9uvR",
      "metadata": {
        "id": "FT3poEHR9uvR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n",
        "\n",
        "def show_model_metrics(train_history, model, val_generator):\n",
        "    # Access training and validation loss/accuracy\n",
        "    train_loss = train_history.history[\"loss\"]\n",
        "    val_loss = train_history.history[\"val_loss\"]\n",
        "    train_acc = train_history.history[\"accuracy\"]\n",
        "    val_acc = train_history.history[\"val_accuracy\"]\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_loss, label=\"Training Loss\")\n",
        "    plt.plot(val_loss, label=\"Validation Loss\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_acc, label=\"Training Accuracy\")\n",
        "    plt.plot(val_acc, label=\"Validation Accuracy\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate model on validation data\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch_x, batch_y in val_generator:\n",
        "        y_true.extend(batch_y)\n",
        "        y_pred_batch = model.predict(batch_x)\n",
        "        y_pred.extend(np.round(y_pred_batch).astype(int))\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Ensure both y_true and y_pred are binary and have the same shape\n",
        "    if y_true.ndim > 1:\n",
        "        y_true = y_true.flatten()\n",
        "    if y_pred.ndim > 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # Trim y_pred to match the length of y_true if needed\n",
        "    if len(y_pred) > len(y_true):\n",
        "        y_pred = y_pred[:len(y_true)]\n",
        "    elif len(y_true) > len(y_pred):\n",
        "        y_true = y_true[:len(y_pred)]\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print('y_true', y_true);\n",
        "    print('y_pred', y_pred);\n",
        "    print(classification_report(y_true, y_pred, target_names=['Normal', 'Violent']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Violent'])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "    average_precision = average_precision_score(y_true, y_pred)\n",
        "\n",
        "    # Plot Precision-Recall curve\n",
        "    plt.figure()\n",
        "    plt.step(recall, precision, where='post', label='Precision-Recall curve (AP = %0.2f)' % average_precision)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.title('Precision-Recall curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_model_metrics(train_history, trained_model, val_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d509bc67e91f8a",
      "metadata": {
        "collapsed": false,
        "id": "58d509bc67e91f8a"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "EQDddyYZz1hj"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}