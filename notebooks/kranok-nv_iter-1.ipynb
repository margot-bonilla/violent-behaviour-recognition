{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Violence Detection Iteration-1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac31ede0095c5e87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constant information to set the data and engineering resources"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a498eb73dee0160"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Path information to load videos and annotations\n",
    "import os\n",
    "\n",
    "ROOT_PATH = r\"C:\\Users\\margo\\OneDrive\\UOC\\projects\\thesis\"\n",
    "DATA_PATH = os.path.join(ROOT_PATH, 'data', 'kranok-nv')\n",
    "ANNOTATIONS_PATH = os.path.join(DATA_PATH, \"Annotations\")\n",
    "VIDEOS_PATH = os.path.join(DATA_PATH, \"Videos\")\n",
    "INFO_PATH = os.path.join(DATA_PATH, \"Info\")\n",
    "\n",
    "MODELS_PATH = os.path.join(ROOT_PATH, 'models')\n",
    "\n",
    "LOGS_PATH = os.path.join(ROOT_PATH, \"logs\")\n",
    "\n",
    "DATASET_TRAIN_PATH = os.path.join(DATA_PATH, INFO_PATH, \"violence_detection_train.hdf5\")\n",
    "DATASET_VAL_PATH = os.path.join(DATA_PATH, INFO_PATH, \"violence_detection_val.hdf5\")\n",
    "DATASET_TEST_PATH = os.path.join(DATA_PATH, INFO_PATH, \"violence_detection_test.hdf5\")\n",
    "\n",
    "# This command is to run all the cells on testing process\n",
    "CREATE_DATASET = False\n",
    "IS_TEST_RUN = False\n",
    "INSTALL_STUFF = False\n",
    "LOAD_MODEL = False\n",
    "\n",
    "EPOCHS = 10\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "MAX_NUM_FRAMES = 15000"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "486bf0f28b9ea3a4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Description\n",
    "\n",
    "* Folder Videos is composed by a list of videos by name: [v1, v2, v3]\n",
    "* Folder Annotations is composed by a list of jsons by name: [v1, v2, v3]\n",
    "\n",
    "The name of each video corresponds to the name of each annotation\n",
    "\n",
    "The annotation json has the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Frame_0000096\": {\n",
    "    \"numberOfPeople\": 1,\n",
    "    \"pedestriansData\": [\n",
    "      [\n",
    "        \"640\",\n",
    "        \"90\",\n",
    "        \"710\",\n",
    "        \"198\",\n",
    "        \"Normal\"\n",
    "      ]\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ffae6659490bae7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load/Extract data\n",
    "\n",
    "I will create a function to extract the data information per each video-annotation pair that will contain the following structure\n",
    "\n",
    "```\n",
    "input: video-name\n",
    "output: VideoInformation\n",
    "\n",
    "class Label(enum):\n",
    "    NORMAL: \"Normal\"\n",
    "    VIOLENT: \"Violent\"\n",
    "\n",
    "class PersonInfo:\n",
    "    ax: int\n",
    "    ay: int\n",
    "    bx: int\n",
    "    by: int\n",
    "    # action of the person\n",
    "    label: LabelEnum\n",
    "\n",
    "class VideoInfo:\n",
    "    frame_name: str\n",
    "    n_people: int\n",
    "    people_info: list[PersonInfo]\n",
    "    label: Label\n",
    "\n",
    "# dictionary of paris by frame_name, frame_numpy_array\n",
    "frame_info: Dict[str, np.ndarray] = dict()\n",
    "``` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7059f845a7a2e8cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "# class syntax\n",
    "class Label(Enum):\n",
    "    VIOLENT = \"Violent\"\n",
    "    NORMAL = \"Normal\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae01b7e638c07ebd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PersonInfo:\n",
    "    def __init__(self, ax: int, ay: int, bx: int, by: int, label: str):\n",
    "        self.ax: int = ax\n",
    "        self.ay: int = ay\n",
    "        self.bx: int = bx\n",
    "        self.by: int = by\n",
    "        self.label: Label = Label(label)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"A: ({self.ax}, {self.ay}), B: ({self.bx}, {self.by}), Label: {self.label.value}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"A: ({self.ax}, {self.ay}), B: ({self.bx}, {self.by}), Label: {self.label.value}\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef71e2cdf21f8c7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VideoInfo:\n",
    "    def __init__(self, category: Label, frame_name: str, n_people: int, people_info: list[PersonInfo]) -> None:\n",
    "        self.frame_name: str = frame_name\n",
    "        self.n_people: int = n_people\n",
    "        self.people_info: List[PersonInfo] = people_info\n",
    "        self.category = category\n",
    "        self.label: int = 1 if category == Label.VIOLENT else 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Category: {self.category.value}, Frame: {self.frame_name}, Persons: {self.n_people}\\n{[ppl for ppl in self.people_info]}\\n\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Category: {self.category.value}, Frame: {self.frame_name}, Persons: {self.n_people}\\n{[ppl for ppl in self.people_info]}\\n\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84293765ed52e58c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test videoInfo class\n",
    "p = PersonInfo(0, 0, 10, 10, \"Violent\")\n",
    "lp = [p]\n",
    "vi = VideoInfo(Label.NORMAL, \"Frame_0001324\", 1, lp)\n",
    "print(vi)\n",
    "vi = VideoInfo(Label.VIOLENT, \"Frame_0001324\", 1, lp)\n",
    "print(vi)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c48b0211df1359f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a03a8ea846e83a6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Store Video Information"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8682d41ce68c059c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Define a dictionary to store frame data and category within HDF5 datasets\n",
    "def create_hdf5_dataset(hdf5_file, name, shape: Tuple = (100, IMG_WIDTH, IMG_HEIGHT, 1), dtype=np.float32):\n",
    "    dataset = hdf5_file.create_dataset(name, shape, dtype=dtype)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adb6c78b5f9b01c6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_video_data(name: str, category: Label, target_size: tuple = (IMG_WIDTH, IMG_HEIGHT)) -> (\n",
    "        List[VideoInfo], Dict[str, np.ndarray]):\n",
    "    annotation_path = os.path.join(ANNOTATIONS_PATH, f\"{name}.json\")\n",
    "    video_path = os.path.join(VIDEOS_PATH, f\"{name}.mp4\")\n",
    "    video_infos = []\n",
    "    frame_infos = {}\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        data_list = json.load(f)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return video_infos, frame_infos\n",
    "\n",
    "    for frame_name in data_list.keys():\n",
    "        n_people = data_list[frame_name][\"numberOfPeople\"]\n",
    "        pedestrians = data_list[frame_name][\"pedestriansData\"]\n",
    "        people_info = []\n",
    "        for ax, ay, bx, by, label in pedestrians:\n",
    "            people_info.append(PersonInfo(ax, ay, bx, by, label))\n",
    "\n",
    "        # Read the frame based on frame name convention: Frame_0000119\n",
    "        frame_id = int(frame_name.split(\"_\")[-1])\n",
    "        ret, frame = cap.read(frame_id)  # Read frame by index\n",
    "\n",
    "        # Normalize the frame\n",
    "        frame = frame.astype(np.float32) / 255.0\n",
    "\n",
    "        # Resize all frames to same dimension\n",
    "        frame = cv2.resize(frame, dsize=target_size)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Include data in storage        \n",
    "        frame_infos[frame_name] = frame\n",
    "        video_infos.append(VideoInfo(category, frame_name, n_people, people_info))\n",
    "\n",
    "    cap.release()\n",
    "    return video_infos, frame_infos\n",
    "\n",
    "\n",
    "# Test function\n",
    "vi, fi = load_video_data(\"Normal_00001\", Label.NORMAL)\n",
    "print(vi)\n",
    "print(fi)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "364459cf357e7c34",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to process a single video and its annotation\n",
    "def process_video(name, category, hdf5_file):\n",
    "    # Load Video Info and Frame Info\n",
    "    video_infos, frame_infos = load_video_data(name, category)\n",
    "    frames_dataset = create_hdf5_dataset(hdf5_file, f\"{name}/frames\",\n",
    "                                         (len(frame_infos), IMG_WIDTH, IMG_HEIGHT, 1))  # Create dataset for frames\n",
    "    labels_dataset = create_hdf5_dataset(hdf5_file, f\"{name}/labels\", (len(frame_infos),),\n",
    "                                         np.integer)  # Create dataset for labels\n",
    "\n",
    "    # Iterate through processed frames\n",
    "    frame_count = 0\n",
    "    for frame_info, video_info in zip(frame_infos.items(), video_infos):\n",
    "        frame_name, frame = frame_info\n",
    "        frame = frame.reshape(IMG_WIDTH, IMG_HEIGHT, 1)  # Reshape to add a single channel for grayscale\n",
    "        frame = frame.reshape(1, IMG_WIDTH, IMG_HEIGHT,\n",
    "                              1)  # Reshape again to add the first dimension for a single frame\n",
    "\n",
    "        # Use indexing with dynamic dimension\n",
    "        frames_dataset[frame_count] = frame\n",
    "        labels_dataset[frame_count] = video_info.label\n",
    "        frame_count += 1\n",
    "\n",
    "    return frames_dataset, labels_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "680a726710172130",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def percentage_processed(l: Label, v_processed: int, n_processed: int, total_processed: int) -> float:\n",
    "    if l == Label.VIOLENT:\n",
    "        return v_processed / total_processed\n",
    "\n",
    "    return n_processed / total_processed\n",
    "\n",
    "\n",
    "def is_train_set(l: Label, v_processed: int, n_processed: int, total_processed: int) -> bool:\n",
    "    return percentage_processed(l, v_processed, n_processed, total_processed) <= 0.8\n",
    "\n",
    "\n",
    "def is_val_set(l: Label, v_processed: int, n_processed: int, total_processed: int) -> bool:\n",
    "    return percentage_processed(l, v_processed, n_processed, total_processed) <= 1\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3704f76967ec1d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "\n",
    "# Function to create HDF5 file (handles potential overwrite)\n",
    "def create_hdf5_file(filepath):\n",
    "    try:\n",
    "        hdf5_file = h5py.File(filepath, \"w\")  # Try creating in write mode\n",
    "        return hdf5_file\n",
    "    except OSError:  # Handle potential overwrite error\n",
    "        os.remove(filepath)  # Remove existing file\n",
    "        print(f\"Removed existing file: {filepath}\")\n",
    "        hdf5_file = h5py.File(filepath, \"w\")  # Retry creating\n",
    "        return hdf5_file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f98b6219f6c141e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "\n",
    "\n",
    "def save_datasets():\n",
    "    # Create empty HDF5 file for each data split (train, validation, test)\n",
    "    train_hdf5_file = create_hdf5_file(DATASET_TRAIN_PATH)\n",
    "    val_hdf5_file = create_hdf5_file(DATASET_VAL_PATH)\n",
    "    # test_hdf5_file = create_hdf5_file(DATASET_TEST_PATH)\n",
    "    try:\n",
    "        # Loop through videos and annotations, processing each and storing in appropriate HDF5 file\n",
    "        videos = os.listdir(str(os.path.join(VIDEOS_PATH)))\n",
    "        violent_videos = [os.path.splitext(v)[0] for v in videos if v.startswith(Label.VIOLENT.value)]\n",
    "        normal_videos = [os.path.splitext(v)[0] for v in videos if v.startswith(Label.NORMAL.value)]\n",
    "\n",
    "        # To balance the data I'll drop some data\n",
    "        minimum_size = min(len(violent_videos), len(normal_videos))\n",
    "\n",
    "        # load less volume for testing purposes\n",
    "        if IS_TEST_RUN:\n",
    "            minimum_size = 10\n",
    "\n",
    "        violent_videos = random.sample(violent_videos, k=minimum_size)\n",
    "        normal_videos = random.sample(normal_videos, k=minimum_size)\n",
    "\n",
    "        total = len(violent_videos) + len(normal_videos)\n",
    "        processed = 0\n",
    "        for violent_video, normal_video in zip(violent_videos, normal_videos):\n",
    "            if processed / total < 0.8:\n",
    "                process_video(violent_video, Label.VIOLENT, train_hdf5_file)\n",
    "                process_video(normal_video, Label.NORMAL, train_hdf5_file)\n",
    "            else:\n",
    "                process_video(violent_video, Label.VIOLENT, val_hdf5_file)\n",
    "                process_video(normal_video, Label.NORMAL, val_hdf5_file)\n",
    "            # else: -- no test yet\n",
    "            #     process_video(name, lab, test_hdf5_file)\n",
    "\n",
    "            print(f'Videos processed: {violent_video} {normal_video}')\n",
    "            print(f'Percentage processed: {(processed / total) * 100}%')\n",
    "            processed += 2\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    finally:\n",
    "        # Close the HDF5 files after processing all videos\n",
    "        train_hdf5_file.close()\n",
    "        val_hdf5_file.close()\n",
    "        # test_hdf5_file.close()\n",
    "\n",
    "    print(\"Datasets are saved in HDF5 files correctly\")\n",
    "    print(f\"Train dataset: {DATASET_TRAIN_PATH}\")\n",
    "    print(f\"Val dataset: {DATASET_VAL_PATH}\")\n",
    "\n",
    "\n",
    "if CREATE_DATASET:\n",
    "    save_datasets()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff44a42cbd0f4b72",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f095eac4f95fc6e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model-1: Simple CNN for Violence Detection\n",
    "This is a basic CNN structure suitable for initial exploration. We'll break it down step-by-step:\n",
    "\n",
    "## Input Layer:\n",
    "Takes a single frame as input, assuming a shape of (224, 224, 1) (grayscale).\n",
    "\n",
    "## Convolutional Layer:\n",
    "Applies a set of filters (kernels) to the input frame to extract features.\n",
    "Typical choices for the first layer could be:\n",
    "\n",
    "* Number of filters: 32\n",
    "* Kernel size: 3x3\n",
    "* Activation function: ReLU (Rectified Linear Unit)\n",
    "\n",
    "## Pooling Layer (Optional):\n",
    "Reduces the dimensionality of the data extracted by the convolutional layer.\n",
    "Options include MaxPooling or AveragePooling with a kernel size of 2x2 and a stride of 2.\n",
    "\n",
    "## Flatten Layer:\n",
    "Converts the output from the convolutional layers (usually a 3D array) into a 1D vector suitable for feeding into a fully-connected layer.\n",
    "\n",
    "## Fully-Connected Layer:\n",
    "Performs classification based on the extracted features.\n",
    "I will use a single neuron with a sigmoid activation for binary classification (violence vs. non-violence)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2c27b9ae9dff6c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Execute this if you are not able to install tensorflow properly\n",
    "if INSTALL_STUFF:\n",
    "    !pip install tensorflow[and-cuda]\n",
    "    !pip install pydot\n",
    "    !pip install graphviz"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8e70cb3dbe0c13d2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "def get_basic_model():\n",
    "    return Sequential([\n",
    "      Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 1)),  # Convolutional layer\n",
    "      Flatten(),\n",
    "      Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "# Visualize model\n",
    "test_model = get_basic_model()\n",
    "test_model.summary()\n",
    "plot_model(test_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "765a498361d55db8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Conv3D, MaxPooling3D, Flatten, LSTM, Dense, Reshape, Dropout\n",
    "\n",
    "def get_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st layer group\n",
    "    model.add(Conv3D(32, (3, 3, 3), strides = 1, input_shape=(MAX_NUM_FRAMES, IMG_WIDTH, IMG_HEIGHT, 1), activation='relu', padding='valid'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "    \n",
    "    model.add(Conv3D(64, (3, 3, 3), activation='relu', strides=1))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "    \n",
    "    model.add(Conv3D(128, (3, 3, 3), activation='relu', strides=1))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=2))\n",
    "    shape = model.layers[-1].output_shape\n",
    "    model.add(Reshape((shape[-1],shape[1]*shape[2]*shape[3])))\n",
    "    \n",
    "    # LSTM - Recurrent Network Layer\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add((Flatten()))\n",
    "    \n",
    "    # FC layers group\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Visualize model\n",
    "test_model = get_lstm_model()\n",
    "test_model.summary()\n",
    "plot_model(test_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3495796141900c30",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46aaa734678e0ef8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b4233be971aeae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to explore the datasets\n",
    "def traverse_datasets(hdf_file):\n",
    "    \"\"\"Traverse all datasets across all groups in HDF5 file.\"\"\"\n",
    "\n",
    "    import h5py\n",
    "\n",
    "    def h5py_dataset_iterator(g, prefix=''):\n",
    "        for key in g.keys():\n",
    "            item = g[key]\n",
    "            path = '{}/{}'.format(prefix, key)\n",
    "            if isinstance(item, h5py.Dataset):  # test for dataset\n",
    "                yield (path, item)\n",
    "            elif isinstance(item, h5py.Group):  # test for group (go down)\n",
    "                yield from h5py_dataset_iterator(item, path)\n",
    "\n",
    "    with h5py.File(hdf_file, 'r') as f:\n",
    "        for (path, dset) in h5py_dataset_iterator(f):\n",
    "            print(path, dset)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Check content of dataset\n",
    "print(\"Checking on the train dataset\")\n",
    "traverse_datasets(DATASET_TRAIN_PATH)\n",
    "print(\"Checking on the val dataset\")\n",
    "traverse_datasets(DATASET_VAL_PATH)\n",
    "# print(\"Checking on the test dataset\")\n",
    "# traverse_datasets(DATASET_TEST_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "606e86302bb90fa5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class HDF5DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator for HDF5 data with video fragments and labels per video.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hdf5_path, batch_size, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=\"binary\"):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.class_mode = class_mode\n",
    "\n",
    "        # Load data information from the HDF5 file\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hdf5_file:\n",
    "            self.num_videos = len(hdf5_file.keys())\n",
    "            print(f\"Found {self.num_videos} videos in {self.hdf5_path}.\")\n",
    "\n",
    "        self.indexes = np.arange(self.num_videos)  # Create index list for shuffling\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches per epoch\n",
    "        return int(np.ceil(self.num_videos / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate a single batch of data and labels\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Load data and labels for the current batch of videos\n",
    "        X, y = self._load_batch_data(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def _load_batch_data(self, indexes):\n",
    "        X = []\n",
    "        y = []\n",
    "        frame_lengths = []\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hdf5_file:\n",
    "            video_names = list(hdf5_file.keys())\n",
    "            for i in indexes:\n",
    "                # Access frames and labels datasets for the specific video\n",
    "                frames = hdf5_file[f\"{video_names[i]}/frames\"][:]\n",
    "                label = hdf5_file[f\"{video_names[i]}/labels\"][:]\n",
    "                \n",
    "                # TODO pad the frame data here to avoid homogenous issue\n",
    "\n",
    "                # Append data and label to batch lists\n",
    "                X.append(frames)\n",
    "                y.append(label)\n",
    "                frame_lengths.append(frames.shape[0])\n",
    "\n",
    "        return np.array(X), np.array(y), np.array(frame_lengths)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # shuffle indexes for each epoch\n",
    "        np.random.shuffle(self.indexes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T18:44:54.813668Z",
     "start_time": "2024-05-05T18:44:54.806808Z"
    }
   },
   "id": "8972781e88090f6e",
   "execution_count": 323
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_dataset_data(dataset_name):\n",
    "    X = []\n",
    "    y = []\n",
    "    frame_lengths = []\n",
    "    with h5py.File(dataset_name, \"r\") as hdf5_file:\n",
    "        video_names = list(hdf5_file.keys())\n",
    "        for video_name in video_names:\n",
    "            # Access frames and labels datasets for the specific video\n",
    "            frames = hdf5_file[f\"{video_name}/frames\"][:]\n",
    "            label = hdf5_file[f\"{video_name}/labels\"][:]\n",
    "            \n",
    "            # TODO pad the frame data here to avoid homogenous issue\n",
    "\n",
    "            # Append data and label to batch lists\n",
    "            X.append(frames)\n",
    "            y.append(label)\n",
    "            frame_lengths.append(frames.shape[0])\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(frame_lengths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T18:58:20.113419Z",
     "start_time": "2024-05-05T18:58:20.108410Z"
    }
   },
   "id": "2bf28ba1094c3035",
   "execution_count": 329
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Difference of number of frames per video\n",
    "max_frames = 0\n",
    "min_frames = 100\n",
    "with h5py.File(DATASET_TRAIN_PATH, \"r\") as hdf5_file:\n",
    "    MAX_FRAGMENT_LENGTH = 500\n",
    "    video_names = list(hdf5_file.keys())\n",
    "    for video in video_names:\n",
    "        n_f = len(hdf5_file[f'{video}/frames'])\n",
    "        max_frames = max(max_frames, n_f)\n",
    "        min_frames = min(min_frames, n_f)\n",
    "print(\"MAX_FRAMES\", max_frames)\n",
    "print(\"MIN FRAMES\", min_frames)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a11113ba633d446",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if INSTALL_STUFF:\n",
    "    !pip install tensorboard\n",
    "\n",
    "# launch this command in your terminal if you want to see the tensorboard\n",
    "# !tensorboard --logdir=C:\\Users\\margo\\OneDrive\\UOC\\projects\\thesis\\logs\n",
    "\n",
    "# Open http://localhost:6006 in your browser to access tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "790a38daaa09f230",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def show_model_metrics(train_history):\n",
    "    # Access training and validation loss/accuracy\n",
    "    train_loss = train_history.history[\"loss\"]\n",
    "    val_loss = train_history.history[\"val_loss\"]\n",
    "    train_acc = train_history.history[\"accuracy\"]\n",
    "    val_acc = train_history.history[\"val_accuracy\"]\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(train_loss, label=\"Training Loss\")\n",
    "    plt.plot(val_loss, label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.plot(train_acc, label=\"Training Accuracy\")\n",
    "    plt.plot(val_acc, label=\"Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f84748e07b7d04b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_model_using_generators(model, epochs=EPOCHS):\n",
    "    \"\"\"Trains a model on the provided data.\n",
    "    \n",
    "    Args:\n",
    "      model: The Keras model to be trained.\n",
    "      epochs: Number of training epochs (default 10).\n",
    "    \n",
    "    Returns:\n",
    "      The trained model.\n",
    "    \"\"\"\n",
    "    # Generators\n",
    "    # Define data generators for training and validation\n",
    "    train_generator = HDF5DataGenerator(DATASET_TRAIN_PATH, BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    val_generator = HDF5DataGenerator(DATASET_VAL_PATH, BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    \n",
    "    \n",
    "    # Declare callbacks\n",
    "    my_callbacks = [\n",
    "        EarlyStopping(patience=2),\n",
    "        ModelCheckpoint(filepath=os.path.join(MODELS_PATH, 'checkpoints', 'model.{epoch:02d}-{val_loss:.2f}.h5')),\n",
    "        TensorBoard(log_dir=LOGS_PATH),\n",
    "    ]\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    train_history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),  # Number of batches per epoch for training data\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=len(val_generator),  # Number of batches per epoch for validation data\n",
    "        callbacks=my_callbacks\n",
    "    )\n",
    "    show_model_metrics(train_history)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T18:55:07.217134Z",
     "start_time": "2024-05-05T18:55:07.212011Z"
    }
   },
   "id": "e4755b9863a69fef",
   "execution_count": 325
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train_model(model, epochs=EPOCHS):\n",
    "    \"\"\"Trains a model on the provided data.\n",
    "    \n",
    "    Args:\n",
    "      model: The Keras model to be trained.\n",
    "      epochs: Number of training epochs (default 10).\n",
    "    \n",
    "    Returns:\n",
    "      The trained model.\n",
    "    \"\"\"\n",
    "    X_train, y_train, n_frames_train = load_dataset_data(DATASET_TRAIN_PATH)\n",
    "    X_val, y_val, n_frames_val = load_dataset_data(DATASET_TRAIN_PATH)\n",
    "    \n",
    "    # Declare callbacks\n",
    "    my_callbacks = [\n",
    "        EarlyStopping(patience=2),\n",
    "        ModelCheckpoint(filepath=os.path.join(MODELS_PATH, 'checkpoints', 'model.{epoch:02d}-{val_loss:.2f}.h5')),\n",
    "        TensorBoard(log_dir=LOGS_PATH),\n",
    "    ]\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    train_history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), callbacks=my_callbacks)\n",
    "    show_model_metrics(train_history)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T18:57:24.063396Z",
     "start_time": "2024-05-05T18:57:24.058886Z"
    }
   },
   "id": "53c64c09edba3938",
   "execution_count": 327
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 28.3 MiB for an array with shape (148, 224, 224, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[331], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m     trained_model \u001B[38;5;241m=\u001B[39m load_model(MODELS_PATH, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbasic_model.keras\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m----> 8\u001B[0m     trained_model \u001B[38;5;241m=\u001B[39m train_model(model, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m     10\u001B[0m model\u001B[38;5;241m.\u001B[39msave(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(MODELS_PATH, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbasic_model.keras\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "Cell \u001B[1;32mIn[327], line 13\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, epochs)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_model\u001B[39m(model, epochs\u001B[38;5;241m=\u001B[39mEPOCHS):\n\u001B[0;32m      4\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Trains a model on the provided data.\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m      The trained model.\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m     X_train, y_train, n_frames_train \u001B[38;5;241m=\u001B[39m load_dataset_data(DATASET_TRAIN_PATH)\n\u001B[0;32m     14\u001B[0m     X_val, y_val, n_frames_val \u001B[38;5;241m=\u001B[39m load_dataset_data(DATASET_TRAIN_PATH)\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;66;03m# Declare callbacks\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[329], line 9\u001B[0m, in \u001B[0;36mload_dataset_data\u001B[1;34m(dataset_name)\u001B[0m\n\u001B[0;32m      6\u001B[0m video_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(hdf5_file\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m video_name \u001B[38;5;129;01min\u001B[39;00m video_names:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Access frames and labels datasets for the specific video\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m     frames \u001B[38;5;241m=\u001B[39m hdf5_file[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/frames\u001B[39m\u001B[38;5;124m\"\u001B[39m][:]\n\u001B[0;32m     10\u001B[0m     label \u001B[38;5;241m=\u001B[39m hdf5_file[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvideo_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/labels\u001B[39m\u001B[38;5;124m\"\u001B[39m][:]\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# TODO pad the frame data here to avoid homogenous issue\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# Append data and label to batch lists\u001B[39;00m\n",
      "File \u001B[1;32mh5py\\_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TFM\\Lib\\site-packages\\h5py\\_hl\\dataset.py:758\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[1;34m(self, args, new_dtype)\u001B[0m\n\u001B[0;32m    756\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fast_read_ok \u001B[38;5;129;01mand\u001B[39;00m (new_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    757\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 758\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fast_reader\u001B[38;5;241m.\u001B[39mread(args)\n\u001B[0;32m    759\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    760\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Fall back to Python read pathway below\u001B[39;00m\n",
      "File \u001B[1;32mh5py\\_selector.pyx:368\u001B[0m, in \u001B[0;36mh5py._selector.Reader.read\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\_selector.pyx:342\u001B[0m, in \u001B[0;36mh5py._selector.Reader.make_array\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 28.3 MiB for an array with shape (148, 224, 224, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Train the model\n",
    "model = get_basic_model()\n",
    "if LOAD_MODEL:\n",
    "    trained_model = load_model(MODELS_PATH, 'basic_model.keras')\n",
    "else:\n",
    "    trained_model = train_model(model, epochs=10)\n",
    "\n",
    "model.save(os.path.join(MODELS_PATH, 'basic_model.keras'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T19:00:32.176688Z",
     "start_time": "2024-05-05T19:00:30.184840Z"
    }
   },
   "id": "612bc4941b179af7",
   "execution_count": 331
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac242e8c24f6326"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
