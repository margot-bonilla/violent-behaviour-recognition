{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/margot-bonilla/violent-behaviour-recognition/blob/master/TFM_2024_Iteration_with_keypoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac31ede0095c5e87",
      "metadata": {
        "collapsed": false,
        "id": "ac31ede0095c5e87"
      },
      "source": [
        "# Violence Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zwTPKaGmmtkI",
      "metadata": {
        "id": "zwTPKaGmmtkI"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "oS-8e9IgbtsD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS-8e9IgbtsD",
        "outputId": "183a3867-6fef-4dac-e41b-bc4572b08529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "D9yzQix5kT01",
      "metadata": {
        "id": "D9yzQix5kT01"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Crate necessary folders for the execution\n",
        "info_path = '/content/Info'\n",
        "if not os.path.exists(info_path):\n",
        "    os.makedirs(info_path)\n",
        "\n",
        "logs_path = '/content/logs'\n",
        "if not os.path.exists(logs_path):\n",
        "    os.makedirs(logs_path)\n",
        "\n",
        "models_path = '/content/models/checkpoints'\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a498eb73dee0160",
      "metadata": {
        "collapsed": false,
        "id": "6a498eb73dee0160"
      },
      "source": [
        "## Constant information to set the data and engineering resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "486bf0f28b9ea3a4",
      "metadata": {
        "id": "486bf0f28b9ea3a4"
      },
      "outputs": [],
      "source": [
        "# Path information to load videos and annotations\n",
        "import os\n",
        "\n",
        "ROOT_PATH = r\"/content/drive/MyDrive/UOC/TFM_2\"\n",
        "ANNOTATIONS_PATH = os.path.join(ROOT_PATH, \"Annotations\")\n",
        "VIDEOS_PATH = os.path.join(ROOT_PATH, \"Videos\")\n",
        "INFO_PATH = \"Info\"\n",
        "MODELS_PATH = 'models'\n",
        "\n",
        "LOGS_PATH = \"logs\"\n",
        "\n",
        "DATASET_TRAIN_PATH = os.path.join(INFO_PATH, \"violence_detection_train.hdf5\")\n",
        "DATASET_VAL_PATH = os.path.join(INFO_PATH, \"violence_detection_val.hdf5\")\n",
        "DATASET_TEST_PATH = os.path.join(INFO_PATH, \"violence_detection_test.hdf5\")\n",
        "\n",
        "## Krakov\n",
        "KRAKOV_VIDEOS_PATH = os.path.join(ROOT_PATH, \"Videos\")\n",
        "INCLUDE_KRAKOV = True\n",
        "\n",
        "## Hockey\n",
        "HOCKEY_VIDEOS_PATH = os.path.join(ROOT_PATH, \"Hockey\", \"data\")\n",
        "INCLUDE_HOCKEY = True\n",
        "\n",
        "## Movies\n",
        "MOVIES_VIDEOS_PATH = os.path.join(ROOT_PATH, \"movies\")\n",
        "INCLUDE_MOVIES = True\n",
        "\n",
        "## RWF 2000\n",
        "RWF2000_VIDEOS_PATH = os.path.join(ROOT_PATH, \"2 - data\")\n",
        "INCLUDE_RWF2000 = True\n",
        "\n",
        "## RWF 2000\n",
        "VIOLENT_VIDEOS_PATH = os.path.join(ROOT_PATH, \"violentFlows\", \"ForTal\")\n",
        "INCLUDE_VIOLENT = True\n",
        "\n",
        "# Set the cells/process you want to run\n",
        "CREATE_DATASET = True\n",
        "PLOT_FRAME_DISTRIBUTION = True\n",
        "IS_TEST_RUN = False\n",
        "TEST_SIZE = 10\n",
        "INSTALL_STUFF = False\n",
        "LOAD_MODEL = False\n",
        "\n",
        "EPOCHS = 10 if IS_TEST_RUN else 70\n",
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "APPLY_GRAYSCALE = True\n",
        "IMG_CHANNELS = 1 if APPLY_GRAYSCALE else 3\n",
        "BODY_KEYPOINTS = 13\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "MAX_NUM_FRAMES = 40\n",
        "APPLY_DATA_AUGMENTATION = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8EzhGB2g0ITu",
      "metadata": {
        "id": "8EzhGB2g0ITu"
      },
      "outputs": [],
      "source": [
        "# global variables\n",
        "# total_videos will contain the full path of each of the videos\n",
        "total_normal_videos = list()\n",
        "total_violent_videos = list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "EtQ8PfPidYsp",
      "metadata": {
        "id": "EtQ8PfPidYsp"
      },
      "outputs": [],
      "source": [
        "# Dedup videos\n",
        "all_video_names = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "XA3O0PEo1NCl",
      "metadata": {
        "id": "XA3O0PEo1NCl"
      },
      "outputs": [],
      "source": [
        "def include_videos(video_path, normal_prefix):\n",
        "  for v in os.listdir(video_path):\n",
        "    total_path = os.path.join(video_path, v)\n",
        "    if v.startswith(normal_prefix) and v not in all_video_names:\n",
        "      total_normal_videos.append(total_path)\n",
        "      all_video_names.add(v)\n",
        "    elif v not in all_video_names:\n",
        "      total_violent_videos.append(total_path)\n",
        "      all_video_names.add(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QTbjRNRP0QyK",
      "metadata": {
        "id": "QTbjRNRP0QyK"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_KRAKOV:\n",
        "  include_videos(os.path.join(KRAKOV_VIDEOS_PATH), 'Normal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "SPHhjSmU1GrU",
      "metadata": {
        "id": "SPHhjSmU1GrU"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_HOCKEY:\n",
        "  include_videos(os.path.join(HOCKEY_VIDEOS_PATH), 'no')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7hv6uyvQ-hQ_",
      "metadata": {
        "id": "7hv6uyvQ-hQ_"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_MOVIES:\n",
        "  for f in os.listdir(MOVIES_VIDEOS_PATH):\n",
        "    folder_path_normal = os.path.join(MOVIES_VIDEOS_PATH, f, \"NonViolence\")\n",
        "    folder_path_violent = os.path.join(MOVIES_VIDEOS_PATH, f, \"Violence\")\n",
        "    for normal in os.listdir(folder_path_normal):\n",
        "      if normal not in all_video_names:\n",
        "        total_normal_videos.append(os.path.join(folder_path_normal, normal))\n",
        "        all_video_names.add(normal)\n",
        "    for violent in os.listdir(folder_path_violent):\n",
        "      if violent not in all_video_names:\n",
        "        total_violent_videos.append(os.path.join(folder_path_violent, violent))\n",
        "        all_video_names.add(violent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "evZje8oUv10h",
      "metadata": {
        "id": "evZje8oUv10h"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_RWF2000:\n",
        "  for f in os.listdir(RWF2000_VIDEOS_PATH):\n",
        "    folder_path_normal = os.path.join(RWF2000_VIDEOS_PATH, f, \"NonFight\")\n",
        "    folder_path_violent = os.path.join(RWF2000_VIDEOS_PATH, f, \"Fight\")\n",
        "    for normal in os.listdir(folder_path_normal):\n",
        "      if normal not in all_video_names:\n",
        "        total_normal_videos.append(os.path.join(folder_path_normal, normal))\n",
        "        all_video_names.add(normal)\n",
        "    for violent in os.listdir(folder_path_violent):\n",
        "      if violent not in all_video_names:\n",
        "        total_violent_videos.append(os.path.join(folder_path_violent, violent))\n",
        "        all_video_names.add(violent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "RpLN9NS7O2JI",
      "metadata": {
        "id": "RpLN9NS7O2JI"
      },
      "outputs": [],
      "source": [
        "if INCLUDE_VIOLENT:\n",
        "  for v in os.listdir(VIOLENT_VIDEOS_PATH):\n",
        "    if v not in all_video_names:\n",
        "      total_violent_videos.append(os.path.join(VIOLENT_VIDEOS_PATH, v))\n",
        "      all_video_names.add(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "TzbHj6cpdR1n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzbHj6cpdR1n",
        "outputId": "9954ea7e-ba59-46bb-9773-bc1f4aa083c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all video names:  6678\n",
            "all dedup video names:  6678\n"
          ]
        }
      ],
      "source": [
        "# Dedup videos\n",
        "print(\"all video names: \", len(all_video_names))\n",
        "print(\"all dedup video names: \", len(total_normal_videos) + len(total_violent_videos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "uYzXJVUCUR6j",
      "metadata": {
        "id": "uYzXJVUCUR6j"
      },
      "outputs": [],
      "source": [
        "# Shuffle videos to increase diversity\n",
        "import random\n",
        "\n",
        "random.shuffle(total_normal_videos)\n",
        "random.shuffle(total_violent_videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "TsRpLMiOwnNA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsRpLMiOwnNA",
        "outputId": "e5f2990c-04f8-4b54-cbca-ffad50d493b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total of normal videos included: 4623\n",
            "Total of violent videos included: 2055\n"
          ]
        }
      ],
      "source": [
        "print(f'Total of normal videos included: {len(total_normal_videos)}')\n",
        "print(f'Total of violent videos included: {len(total_violent_videos)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "56nWT2tBRpCn",
      "metadata": {
        "id": "56nWT2tBRpCn"
      },
      "outputs": [],
      "source": [
        "# Cut the volume of videos for testing purposes\n",
        "\n",
        "if IS_TEST_RUN:\n",
        "  total_normal_videos = total_normal_videos[:TEST_SIZE + 5] # to handle inbalance\n",
        "  total_violent_videos = total_violent_videos[:TEST_SIZE]\n",
        "  print(f'Total normal videos: {len(total_normal_videos)}')\n",
        "  print(f'Total violent videos: {len(total_violent_videos)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z8Vr9PmBh-bC",
      "metadata": {
        "id": "z8Vr9PmBh-bC"
      },
      "source": [
        "## Libraries that you may need to run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "j21fk1tsh7DD",
      "metadata": {
        "id": "j21fk1tsh7DD"
      },
      "outputs": [],
      "source": [
        "# Execute this if you are not able to install tensorflow properly\n",
        "if INSTALL_STUFF:\n",
        "    !pip install tensorflow[and-cuda]\n",
        "    !pip install pydot\n",
        "    !pip install graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EQDddyYZz1hj",
      "metadata": {
        "id": "EQDddyYZz1hj"
      },
      "source": [
        "## Plot distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "g78SxdjzqHQb",
      "metadata": {
        "id": "g78SxdjzqHQb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy import stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import progressbar\n",
        "\n",
        "def create_frame_distribution(videos):\n",
        "    number_of_frames = list()\n",
        "    bar = progressbar.ProgressBar(maxval=len(videos), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    bar.start()\n",
        "    processed = 0\n",
        "    for video in videos:\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video)\n",
        "            property_id = int(cv2.CAP_PROP_FRAME_COUNT)\n",
        "            length = int(cv2.VideoCapture.get(cap, property_id))\n",
        "            number_of_frames.append(length)\n",
        "            cap.release()\n",
        "\n",
        "            processed += 1\n",
        "            bar.update(processed)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {e.message}\")\n",
        "    bar.finish()\n",
        "\n",
        "    return number_of_frames\n",
        "\n",
        "def plot_frame_distribution(number_of_frames):\n",
        "    frame_counts = np.array(number_of_frames)\n",
        "    print(\"Number of unique frame counts:\", len(frame_counts))\n",
        "    print(\"Max frames in a video:\", frame_counts.max())\n",
        "    print(\"Min frames in a video:\", frame_counts.min())\n",
        "    print(\"Average frames per video:\", frame_counts.mean())\n",
        "    print(\"Median frames per video:\", np.median(frame_counts))\n",
        "    mode_value = st.mode(frame_counts).mode\n",
        "    print(\"Mode frames per video:\", mode_value)\n",
        "    print(\"Videos over 2k frames:\", np.sum(frame_counts > 2000))\n",
        "    print(\"Videos over 5k frames:\", np.sum(frame_counts > 5000))\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(frame_counts, bins=100, color='skyblue', edgecolor='black')\n",
        "\n",
        "    # Add vertical lines for mean, median, and mode\n",
        "    plt.axvline(frame_counts.mean(), color='red', linestyle='dashed', linewidth=1, label='Mean')\n",
        "    plt.axvline(np.median(frame_counts), color='yellow', linestyle='dashed', linewidth=1, label='Median')\n",
        "    plt.axvline(mode_value, color='green', linestyle='dashed', linewidth=1, label='Mode')\n",
        "\n",
        "    # Highlight majority population range (for example, mean Â± one standard deviation)\n",
        "    lower_bound = frame_counts.mean() - frame_counts.std()\n",
        "    upper_bound = frame_counts.mean() + frame_counts.std()\n",
        "    print(f\"Majority Range: {lower_bound} - {upper_bound}\")\n",
        "    plt.axvspan(lower_bound, upper_bound, color='orange', alpha=0.2, label='Majority Range')\n",
        "\n",
        "    plt.title('Distribution of Number of Frames per Video')\n",
        "    plt.xlabel('Number of Frames')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LqW_s8Kfz6U5",
      "metadata": {
        "id": "LqW_s8Kfz6U5"
      },
      "outputs": [],
      "source": [
        "if PLOT_FRAME_DISTRIBUTION:\n",
        "  number_of_frames = create_frame_distribution(total_normal_videos + total_violent_videos)\n",
        "  plot_frame_distribution(number_of_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fD81d8w8foby",
      "metadata": {
        "id": "fD81d8w8foby"
      },
      "source": [
        "# Process video data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae01b7e638c07ebd",
      "metadata": {
        "id": "ae01b7e638c07ebd"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "# class syntax\n",
        "class Label(Enum):\n",
        "    VIOLENT = \"Violent\"\n",
        "    NORMAL = \"Normal\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lTYcn47Dxy9u",
      "metadata": {
        "collapsed": true,
        "id": "lTYcn47Dxy9u"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XbkXOeyjLLb_",
      "metadata": {
        "id": "XbkXOeyjLLb_"
      },
      "source": [
        "## Image functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49ad848fd4d8445",
      "metadata": {
        "id": "c49ad848fd4d8445"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import h5py\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "def duplicate_frames(frames, target_count):\n",
        "  # Duplicate the last available frame to fill missing frames\n",
        "  last_frame = frames[-1]\n",
        "  while len(frames) < target_count:\n",
        "      frames.append(last_frame.copy())\n",
        "  return frames\n",
        "\n",
        "\n",
        "def normalize(frame):\n",
        "  # Normalize the frame\n",
        "  frame = frame.astype(np.float64)\n",
        "  frame -= frame.min()\n",
        "  frame /= frame.max()\n",
        "\n",
        "  frame *= 255 # [0, 255] range\n",
        "\n",
        "  return frame.astype(np.uint8)\n",
        "\n",
        "def resize(frame, target_size=(IMG_WIDTH, IMG_HEIGHT)):\n",
        "  # Resize all frames to same dimension\n",
        "  frame = cv2.resize(frame, dsize=target_size)\n",
        "\n",
        "  return frame\n",
        "\n",
        "def convert_grayscale(frame):\n",
        "  # Convert to grayscale\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  return frame\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sSSQK6zELT4u",
      "metadata": {
        "id": "sSSQK6zELT4u"
      },
      "source": [
        "## Keypoints functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45Offj5hLvb_",
      "metadata": {
        "id": "45Offj5hLvb_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_distances(keypoints):\n",
        "    keypoints = np.array(keypoints)\n",
        "    num_keypoints = keypoints.shape[0]\n",
        "    distances = []\n",
        "\n",
        "    for i in range(num_keypoints):\n",
        "        for j in range(i + 1, num_keypoints):\n",
        "            distance = np.linalg.norm(keypoints[i] - keypoints[j])\n",
        "            distances.append(distance)\n",
        "    return distances\n",
        "\n",
        "def calculate_velocity(keypoints_prev, keypoints_curr, fps):\n",
        "    if keypoints_prev is None:\n",
        "        return None\n",
        "    velocity = (np.array(keypoints_curr) - np.array(keypoints_prev)) * fps\n",
        "    return velocity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e-KSIDXqKajn",
      "metadata": {
        "id": "e-KSIDXqKajn"
      },
      "outputs": [],
      "source": [
        "def draw_keypoints(pose, mp_pose, frame):\n",
        "  frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  results = pose.process(frame_rgb)\n",
        "  keypoints = np.zeros((BODY_KEYPOINTS, 3), dtype=np.float32)\n",
        "\n",
        "  mp_drawing = mp.solutions.drawing_utils\n",
        "  mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "  custom_style = mp_drawing_styles.get_default_pose_landmarks_style()\n",
        "  custom_connections = list(mp_pose.POSE_CONNECTIONS)\n",
        "\n",
        "  # list of landmarks to exclude from the drawing\n",
        "  excluded_landmarks = [\n",
        "      PoseLandmark.LEFT_EYE,\n",
        "      PoseLandmark.RIGHT_EYE,\n",
        "      PoseLandmark.LEFT_EYE_INNER,\n",
        "      PoseLandmark.RIGHT_EYE_INNER,\n",
        "      PoseLandmark.LEFT_EAR,\n",
        "      PoseLandmark.RIGHT_EAR,\n",
        "      PoseLandmark.LEFT_EYE_OUTER,\n",
        "      PoseLandmark.RIGHT_EYE_OUTER,\n",
        "      PoseLandmark.NOSE,\n",
        "      PoseLandmark.MOUTH_LEFT,\n",
        "      PoseLandmark.MOUTH_RIGHT ]\n",
        "\n",
        "  for landmark in excluded_landmarks:\n",
        "      # we change the way the excluded landmarks are drawn\n",
        "      custom_style[landmark] = DrawingSpec(color=(255,255,0), thickness=None)\n",
        "      # we remove all connections which contain these landmarks\n",
        "      custom_connections = [connection_tuple for connection_tuple in custom_connections\n",
        "                              if landmark.value not in connection_tuple]\n",
        "\n",
        "  if results.pose_landmarks:\n",
        "    mp_drawing.draw_landmarks(\n",
        "        frame,\n",
        "        results.pose_landmarks,\n",
        "        connections = custom_connections, #  passing the modified connections list\n",
        "        landmark_drawing_spec=custom_style) # and drawing style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "buMG0owh0NFK",
      "metadata": {
        "id": "buMG0owh0NFK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from mediapipe.python.solutions.pose import PoseLandmark\n",
        "from mediapipe.python.solutions.drawing_utils import DrawingSpec\n",
        "\n",
        "def extract_pose_keypoints(pose, mp_pose, frame):\n",
        "  \"\"\"\n",
        "  Extracts keypoints from a video frame using MediaPipe Pose.\n",
        "  Only keypoints with a confidence above the specified threshold are retained.\n",
        "  \"\"\"\n",
        "  frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  results = pose.process(frame_rgb)\n",
        "  keypoints = np.zeros((BODY_KEYPOINTS, 3), dtype=np.float32)\n",
        "\n",
        "  if results.pose_landmarks:\n",
        "    person_landmarks = results.pose_landmarks\n",
        "\n",
        "    # Define the keypoints to be extracted based on their index in MediaPipe's output\n",
        "    landmark_indices = [\n",
        "        mp_pose.PoseLandmark.NOSE,\n",
        "        mp_pose.PoseLandmark.LEFT_SHOULDER ,\n",
        "        mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
        "        mp_pose.PoseLandmark.LEFT_ELBOW,\n",
        "        mp_pose.PoseLandmark.RIGHT_ELBOW,\n",
        "        mp_pose.PoseLandmark.LEFT_WRIST,\n",
        "        mp_pose.PoseLandmark.RIGHT_WRIST,\n",
        "        mp_pose.PoseLandmark.LEFT_HIP,\n",
        "        mp_pose.PoseLandmark.RIGHT_HIP,\n",
        "        mp_pose.PoseLandmark.LEFT_KNEE,\n",
        "        mp_pose.PoseLandmark.RIGHT_KNEE,\n",
        "        mp_pose.PoseLandmark.LEFT_ANKLE,\n",
        "        mp_pose.PoseLandmark.RIGHT_ANKLE,\n",
        "    ]\n",
        "\n",
        "    for idx, lm_idx in enumerate(landmark_indices):\n",
        "      landmark = person_landmarks.landmark[lm_idx]\n",
        "      x, y, z = landmark.x, landmark.y, landmark.z\n",
        "\n",
        "      # Check for NaNs and handle them by setting to zero\n",
        "      if np.isnan(x) or np.isnan(y) or np.isnan(z):\n",
        "        print(f\"keypoint contains NaN x: {x}, y: {y}, z: {z}\")\n",
        "        keypoints[idx] = (0, 0, 0)\n",
        "      else:\n",
        "        keypoints[idx] = (x, y, z)\n",
        "\n",
        "  return keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hc0Y-tElLZEX",
      "metadata": {
        "id": "hc0Y-tElLZEX"
      },
      "source": [
        "## Process and Save videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6Bx4wOQBToY",
      "metadata": {
        "id": "b6Bx4wOQBToY"
      },
      "outputs": [],
      "source": [
        "def save_to_hdf5(hdf5_file, video_name, frames, keypoints, label):\n",
        "  with h5py.File(hdf5_file, 'a') as f:\n",
        "    # Check if the group for the video already exists\n",
        "    if video_name in f:\n",
        "      # Delete existing datasets within the group (if any)\n",
        "      del f[f'{video_name}']\n",
        "    # Create a new group for the video data\n",
        "    group = f.create_group(video_name)\n",
        "\n",
        "    # Save frames, keypoints, and label as datasets within the group with compression\n",
        "    group.create_dataset('frames', data=frames, compression=\"gzip\")\n",
        "    group.create_dataset('keypoints', data=keypoints, compression=\"gzip\")\n",
        "    group.create_dataset('label', data=np.array([label]), dtype='i')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680a726710172130",
      "metadata": {
        "id": "680a726710172130"
      },
      "outputs": [],
      "source": [
        "# Function to process a single video and its annotation\n",
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def process_video(video_path, hdf5_file, video_name, label, max_frames):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  frames = []\n",
        "  keypoints_list = []\n",
        "\n",
        "  mp_pose = mp.solutions.pose\n",
        "  mp_drawing = mp.solutions.drawing_utils\n",
        "  pose = mp_pose.Pose(\n",
        "      model_complexity=0,\n",
        "      min_detection_confidence=0.6\n",
        "  )\n",
        "\n",
        "  while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    # Resize frame\n",
        "    frame = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    frame = normalize(frame)\n",
        "\n",
        "    # Extract keypoints after image size modifications\n",
        "    keypoints = extract_pose_keypoints(pose, mp_pose, frame)\n",
        "    keypoints_list.append(keypoints)\n",
        "\n",
        "    # Convert to grayscale (apply image color)\n",
        "    if APPLY_GRAYSCALE:\n",
        "      frame = convert_grayscale(frame)\n",
        "\n",
        "    frames.append(frame)\n",
        "\n",
        "  cap.release()\n",
        "  pose.close()\n",
        "\n",
        "  # Ensure consistency frame length (padding or truncating)\n",
        "  num_frames = len(frames)\n",
        "  if num_frames > max_frames:\n",
        "    segment_count = num_frames // max_frames\n",
        "    for i in range(segment_count):\n",
        "      segment_frames = frames[i * max_frames:(i + 1) * max_frames]\n",
        "      segment_keypoints = keypoints_list[i * max_frames:(i + 1) * max_frames]\n",
        "      save_to_hdf5(hdf5_file, f'{video_name}_segment_{i}', segment_frames, segment_keypoints, label)\n",
        "\n",
        "    return\n",
        "  save_to_hdf5(hdf5_file, video_name, frames, keypoints_list, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daszAff5B1uL",
      "metadata": {
        "collapsed": true,
        "id": "daszAff5B1uL"
      },
      "outputs": [],
      "source": [
        "# Test save video with frames, keypoints, and label\n",
        "video_path = '/content/drive/MyDrive/UOC/TFM_2/Videos/Violent_00324.mp4'\n",
        "hdf5_file = 'test.hdf5'\n",
        "video_name = 'Violent_00324'\n",
        "label = 1\n",
        "process_video(video_path, hdf5_file, video_name, label, MAX_NUM_FRAMES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q1qNND7rRX9o",
      "metadata": {
        "id": "q1qNND7rRX9o"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "def explore_hdf5_file(hdf5_file_path):\n",
        "    def print_attrs(name, obj):\n",
        "        print(f\"{name}: {obj}\")\n",
        "        if isinstance(obj, h5py.Dataset):\n",
        "            print(f\"  Shape: {obj.shape}\")\n",
        "            print(f\"  Data type: {obj.dtype}\")\n",
        "        elif isinstance(obj, h5py.Group):\n",
        "            print(f\"  Contains: {list(obj.keys())}\")\n",
        "\n",
        "    with h5py.File(hdf5_file_path, 'r') as f:\n",
        "        f.visititems(print_attrs)\n",
        "\n",
        "# Example usage\n",
        "# print(\"Test file contents:\")\n",
        "# explore_hdf5_file(hdf5_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hRB9y2rJqO_B",
      "metadata": {
        "id": "hRB9y2rJqO_B"
      },
      "outputs": [],
      "source": [
        "import progressbar\n",
        "\n",
        "def process_and_save_videos(videos, train_file, val_file, batch_size=BATCH_SIZE, max_frames=MAX_NUM_FRAMES):\n",
        "  \"\"\"\n",
        "  viceos: Tuple: (video_path: string, label: Label)\n",
        "  train_file: hdf5 file for train dataset\n",
        "  val_file: hdf5 file for val dataset\n",
        "  \"\"\"\n",
        "  def split_videos(vieos, split_ratio=0.8):\n",
        "    train_videos = []\n",
        "    val_videos = []\n",
        "    for i, (video_path, label) in enumerate(videos):\n",
        "      video_name = video_path.split('/')[-1].split('.')[0]\n",
        "      if i / len(videos) < split_ratio:\n",
        "        train_videos.append((video_path, video_name, label))\n",
        "      else:\n",
        "        val_videos.append((video_path, video_name, label))\n",
        "\n",
        "    return train_videos, val_videos\n",
        "\n",
        "  train_videos, val_videos = split_videos(videos)\n",
        "\n",
        "  def process_with_progress(videos, hdf5_file):\n",
        "    bar = progressbar.ProgressBar(maxval=len(videos), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    bar.start()\n",
        "    for i, (video_path, video_name, label) in enumerate(videos):\n",
        "      process_video(video_path, hdf5_file, video_name, label, max_frames)\n",
        "      bar.update(i)\n",
        "    bar.finish()\n",
        "\n",
        "  # Process train videos\n",
        "  process_with_progress(train_videos, train_file)\n",
        "\n",
        "  # Process validation videos\n",
        "  process_with_progress(val_videos, val_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff44a42cbd0f4b72",
      "metadata": {
        "id": "ff44a42cbd0f4b72"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def save_dataset(all_videos):\n",
        "    # Process normal videos\n",
        "    process_and_save_videos(all_videos, DATASET_TRAIN_PATH, DATASET_VAL_PATH)\n",
        "\n",
        "    print(\"Datasets are saved in HDF5 files correctly\")\n",
        "    print(f\"Train dataset: {DATASET_TRAIN_PATH}\")\n",
        "    print(f\"Val dataset: {DATASET_VAL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wYC--4WQi_Wh",
      "metadata": {
        "id": "wYC--4WQi_Wh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "# label all normal and violent videos and shuffle them\n",
        "all_videos = list()\n",
        "for normal in total_normal_videos:\n",
        "  all_videos.append((normal, 0))\n",
        "for violent in total_violent_videos:\n",
        "  all_videos.append((violent, 1))\n",
        "\n",
        "random.shuffle(all_videos)\n",
        "save_dataset(all_videos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606e86302bb90fa5",
      "metadata": {
        "collapsed": true,
        "id": "606e86302bb90fa5"
      },
      "outputs": [],
      "source": [
        "# Function to explore the datasets\n",
        "def traverse_datasets(hdf_file):\n",
        "    \"\"\"Traverse all datasets across all groups in HDF5 file.\"\"\"\n",
        "\n",
        "    import h5py\n",
        "\n",
        "    def h5py_dataset_iterator(g, prefix=''):\n",
        "        for key in g.keys():\n",
        "            item = g[key]\n",
        "            path = '{}/{}'.format(prefix, key)\n",
        "            if isinstance(item, h5py.Dataset):  # test for dataset\n",
        "                yield (path, item)\n",
        "            elif isinstance(item, h5py.Group):  # test for group (go down)\n",
        "                yield from h5py_dataset_iterator(item, path)\n",
        "\n",
        "    with h5py.File(hdf_file, 'r') as f:\n",
        "        print(f'Number of keys {len(f.keys())}\\n')\n",
        "        for (path, dset) in h5py_dataset_iterator(f):\n",
        "            print(path, dset)\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46aaa734678e0ef8",
      "metadata": {
        "collapsed": false,
        "id": "46aaa734678e0ef8"
      },
      "source": [
        "# Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yZYJt1Io6kCw",
      "metadata": {
        "id": "yZYJt1Io6kCw"
      },
      "source": [
        "## Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9-Z-BCoW6i-O",
      "metadata": {
        "id": "9-Z-BCoW6i-O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_keypoints(frame, keypoints):\n",
        "    plt.imshow(frame)\n",
        "    for keypoint in keypoints:\n",
        "        x, y, z = keypoint\n",
        "        plt.scatter(x * frame.shape[1], y * frame.shape[0], c='r', s=10)\n",
        "    plt.title('Frame with Keypoints')\n",
        "    plt.show()\n",
        "\n",
        "def check_generator_output(generator, batch_index=0):\n",
        "    # Get the batch from the generator\n",
        "    (frames_batch, keypoints_batch), labels_batch = generator[batch_index]\n",
        "\n",
        "    print(f\"Batch shape (frames): {frames_batch.shape}\")\n",
        "    print(f\"Batch shape (keypoints): {keypoints_batch.shape}\")\n",
        "    print(f\"Batch shape (labels): {labels_batch.shape}\")\n",
        "\n",
        "    # Print out the first frame and corresponding keypoints from the first sample in the batch\n",
        "    first_frame = frames_batch[0][0]\n",
        "    first_keypoints = keypoints_batch[0][0]\n",
        "\n",
        "    # Plot the first frame\n",
        "    plt.imshow(first_frame)\n",
        "    for x, y, _ in first_keypoints:\n",
        "        plt.scatter(x, y, c='red')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot keypoints on the first frame of the first sample\n",
        "    plot_keypoints(first_frame, first_keypoints)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Byb9qOHw6sds",
      "metadata": {
        "id": "Byb9qOHw6sds"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_frames(generator, num_batches=1, num_frames=5):\n",
        "    for i in range(num_batches):\n",
        "        X, y = generator[i]\n",
        "        for j in range(num_frames):\n",
        "            plt.imshow(X[j][0, :, :, 0])\n",
        "            plt.title(f\"Label: {y[j]}\")\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UTIbkRn56sTV",
      "metadata": {
        "id": "UTIbkRn56sTV"
      },
      "outputs": [],
      "source": [
        "def check_class_distribution(generator):\n",
        "    all_labels = []\n",
        "    for i in range(len(generator)):\n",
        "        _, y = generator[i]\n",
        "        all_labels.extend(y)\n",
        "\n",
        "    unique, counts = np.unique(all_labels, return_counts=True)\n",
        "    label_distribution = dict(zip(unique, counts))\n",
        "\n",
        "    print(\"Class Distribution:\")\n",
        "    for label, count in label_distribution.items():\n",
        "        print(f\" - Label {label}: {count} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8972781e88090f6e",
      "metadata": {
        "collapsed": true,
        "id": "8972781e88090f6e"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class HDF5DataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Custom data generator for HDF5 data with video fragments and labels per video.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_file, batch_size, target_size, max_frames, augmentation=None):\n",
        "        self.hdf5_file = hdf5_file\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.max_frames = max_frames\n",
        "        self.augmentation = augmentation\n",
        "        self.indices = self._get_indices()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _get_indices(self):\n",
        "        # Load data information from the HDF5 file\n",
        "        with h5py.File(self.hdf5_file, \"r\") as f:\n",
        "            return list(f.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of batches per epoch\n",
        "        return int(np.floor(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate a single batch of data and labels\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Load data and labels for the current batch of videos\n",
        "        X, X_keypoints, y = self.__data_generation(batch_indices)\n",
        "\n",
        "        return (X, X_keypoints), y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle indices for each epoch\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, batch_indices):\n",
        "      # Initialize arrays for frames, keypoints, and labels\n",
        "      X = np.zeros((self.batch_size, self.max_frames, *self.target_size, IMG_CHANNELS), dtype=np.float32)\n",
        "      X_keypoints = np.zeros((self.batch_size, self.max_frames, BODY_KEYPOINTS, 3), dtype=np.float32)\n",
        "      y = np.zeros((self.batch_size,), dtype=np.float32)\n",
        "\n",
        "      with h5py.File(self.hdf5_file, 'r') as f:\n",
        "        for i, idx in enumerate(batch_indices):\n",
        "          frames = f[f'{idx}/frames'][:]\n",
        "          keypoints = f[f'{idx}/keypoints'][:]\n",
        "          label = f[f'{idx}/label'][()]\n",
        "\n",
        "\n",
        "          if APPLY_GRAYSCALE:\n",
        "            frames = np.expand_dims(frames, axis=-1)  # Add channel dimension\n",
        "\n",
        "\n",
        "          # Check for NaNs in the original frames and keypoints\n",
        "          if np.isnan(frames).any() or np.isnan(keypoints).any():\n",
        "            print(f\"Warning: Found NaN in frames or keypoints for index {idx}. Replacing with default values.\")\n",
        "            frames = np.zeros_like(frames)\n",
        "            keypoints = np.zeros_like(keypoints)\n",
        "\n",
        "          # Pad or truncate frames to max_frames\n",
        "          if frames.shape[0] < self.max_frames:\n",
        "            pad_width = self.max_frames - frames.shape[0]\n",
        "            # Pad with copies of the last frame\n",
        "            last_frame = frames[-1]\n",
        "            last_keypoints = keypoints[-1]\n",
        "\n",
        "            print(\"Shape of frames before concatenation:\", frames.shape)  # Add this line\n",
        "            print(\"Shape of last_frame:\", last_frame.shape)  # Add this line\n",
        "\n",
        "            # # Modify the padding logic for frames:\n",
        "            # if pad_width > 0:\n",
        "            #     last_frame_padded = np.tile(last_frame, (pad_width, 1, 1))  # Remove np.expand_dims here\n",
        "            #     frames = np.concatenate([frames, last_frame_padded], axis=0)\n",
        "\n",
        "            frames = np.concatenate([frames, np.tile(last_frame, (pad_width, 1, 1, 1))], axis=0)\n",
        "            keypoints = np.concatenate([keypoints, np.tile(last_keypoints, (pad_width, 1, 1))], axis=0)\n",
        "          elif frames.shape[0] > self.max_frames:\n",
        "            frames = frames[:self.max_frames]\n",
        "            keypoints = keypoints[:self.max_frames]\n",
        "\n",
        "          if self.augmentation:\n",
        "              augmented_frames = []\n",
        "              for frame in frames:\n",
        "                  augmented_frame = self.augmentation.random_transform(frame)\n",
        "                  augmented_frames.append(augmented_frame)\n",
        "              frames = np.array(augmented_frames)\n",
        "\n",
        "          X[i] = frames\n",
        "          X_keypoints[i] = keypoints\n",
        "          y[i] = label\n",
        "\n",
        "      return X, X_keypoints, y\n",
        "\n",
        "\n",
        "# Create data generators\n",
        "# train_generator = HDF5DataGenerator(DATASET_TRAIN_PATH, batch_size=8, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=MAX_NUM_FRAMES)\n",
        "# val_generator = HDF5DataGenerator(DATASET_VAL_PATH, batch_size=8, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=MAX_NUM_FRAMES)\n",
        "test_generator = HDF5DataGenerator('test.hdf5', batch_size=8, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=MAX_NUM_FRAMES)\n",
        "\n",
        "# Example function to check the generator output\n",
        "def check_generator_output(generator, batch_index=0):\n",
        "    (frames_batch, keypoints_batch), labels_batch = generator[batch_index]\n",
        "\n",
        "    print(f\"Batch shape (frames): {frames_batch.shape}\")\n",
        "    print(f\"Batch shape (keypoints): {keypoints_batch.shape}\")\n",
        "    print(f\"Batch shape (labels): {labels_batch.shape}\")\n",
        "\n",
        "    # Optionally visualize the first frame and keypoints\n",
        "    first_frame = frames_batch[0][0]\n",
        "    first_keypoints = keypoints_batch[0][0]\n",
        "\n",
        "    plt.imshow(first_frame.astype(np.uint8))\n",
        "    for x, y, _ in first_keypoints:\n",
        "        plt.scatter(x, y, c='red')\n",
        "    plt.show()\n",
        "\n",
        "# Check the generator output\n",
        "check_generator_output(test_generator, batch_index=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b4233be971aeae",
      "metadata": {
        "collapsed": false,
        "id": "c0b4233be971aeae"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf28ba1094c3035",
      "metadata": {
        "id": "2bf28ba1094c3035"
      },
      "outputs": [],
      "source": [
        "def load_dataset_data(dataset_name):\n",
        "    X = []\n",
        "    y = []\n",
        "    frame_lengths = []\n",
        "    with h5py.File(dataset_name, \"r\") as hdf5_file:\n",
        "        video_names = list(hdf5_file.keys())\n",
        "        for video_name in video_names:\n",
        "            # Access frames and labels datasets for the specific video\n",
        "            frames = hdf5_file[f\"{video_name}/frames\"][:]\n",
        "            labels = hdf5_file[f\"{video_name}/labels\"][:]\n",
        "\n",
        "            # Append data and label to batch lists\n",
        "            X.append(frames)\n",
        "            y.append(labels)\n",
        "            frame_lengths.append(frames.shape[0])\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(frame_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vA1NhUnRoojU",
      "metadata": {
        "collapsed": true,
        "id": "vA1NhUnRoojU"
      },
      "outputs": [],
      "source": [
        "def get_dataset_size(dataset_name):\n",
        "  size = 0\n",
        "  with h5py.File(dataset_name, 'r') as f:\n",
        "    video_names = list(f.keys())\n",
        "    for v in video_names:\n",
        "      size += len(f[f'{v}/frames'])\n",
        "\n",
        "  return size\n",
        "\n",
        "get_dataset_size(DATASET_TRAIN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790a38daaa09f230",
      "metadata": {
        "collapsed": true,
        "id": "790a38daaa09f230"
      },
      "outputs": [],
      "source": [
        "if INSTALL_STUFF:\n",
        "    !pip install tensorboard\n",
        "\n",
        "# launch this command in your terminal if you want to see the tensorboard\n",
        "# !tensorboard --logdir=C:\\Users\\margo\\OneDrive\\UOC\\projects\\thesis\\logs\n",
        "\n",
        "# Open http://localhost:6006 in your browser to access tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T1iC8THk6n7S",
      "metadata": {
        "id": "T1iC8THk6n7S"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XnuwRMnTx8tD",
      "metadata": {
        "id": "XnuwRMnTx8tD"
      },
      "source": [
        "### Training using generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4755b9863a69fef",
      "metadata": {
        "id": "e4755b9863a69fef"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "\n",
        "import os\n",
        "\n",
        "def train_model_using_generators(model, epochs=EPOCHS):\n",
        "    \"\"\"Trains a model on the provided data using data generators.\n",
        "\n",
        "    Args:\n",
        "      model: The Keras model to be trained.\n",
        "      epochs: Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "      The trained model, train history, and val generator\n",
        "    \"\"\"\n",
        "\n",
        "    # Generators\n",
        "    train_generator = HDF5DataGenerator(DATASET_TRAIN_PATH, BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=MAX_NUM_FRAMES)\n",
        "    val_generator = HDF5DataGenerator(DATASET_VAL_PATH, BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH), max_frames=MAX_NUM_FRAMES)\n",
        "\n",
        "    # Callbacks\n",
        "    my_callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ModelCheckpoint(filepath=os.path.join(MODELS_PATH, 'checkpoints', 'model.{epoch:02d}-{val_loss:.2f}.keras'),\n",
        "                        save_best_only=True),\n",
        "        TensorBoard(log_dir=LOGS_PATH),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "    ]\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    train_history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_generator,\n",
        "        callbacks=my_callbacks\n",
        "    )\n",
        "\n",
        "    return model, train_history, val_generator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4LbiaqYWhHI2",
      "metadata": {
        "id": "4LbiaqYWhHI2"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0854e0938839272",
      "metadata": {
        "id": "c0854e0938839272"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv3D, Conv2D, MaxPooling2D, TimeDistributed, Flatten, LSTM, Dropout, Dense, BatchNormalization, MaxPooling3D\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "import os\n",
        "\n",
        "def create_video_model_deprecated(input_shape=(MAX_NUM_FRAMES, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), num_classes=1):\n",
        "    model = Sequential([\n",
        "        Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling3D((1, 2, 2)),  # Pool only in spatial dimensions\n",
        "\n",
        "        Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling3D((1, 2, 2)),  # Pool only in spatial dimensions\n",
        "\n",
        "        Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling3D((1, 2, 2)),  # Pool only in spatial dimensions\n",
        "\n",
        "        Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling3D((1, 2, 2)),  # Pool only in spatial dimensions\n",
        "\n",
        "        TimeDistributed(Flatten()),  # Flatten the output of each time step\n",
        "        LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)),  # L2 regularization\n",
        "        Dropout(0.5),  # Dropout to prevent overfitting\n",
        "\n",
        "        LSTM(32, kernel_regularizer=l2(0.01)),  # Another LSTM layer for added depth\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='sigmoid')  # Sigmoid for binary classification\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the video model\n",
        "video_model_deprecated = create_video_model_deprecated()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QonHD_frYjoq",
      "metadata": {
        "id": "QonHD_frYjoq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv3D, BatchNormalization, MaxPooling3D, TimeDistributed, Flatten, LSTM, Dropout, Dense, concatenate\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def create_video_model(input_shape_frames=(MAX_NUM_FRAMES, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
        "                       input_shape_keypoints=(MAX_NUM_FRAMES, BODY_KEYPOINTS, 3), num_classes=1):\n",
        "\n",
        "    # Frame input branch\n",
        "    frames_input = Input(shape=input_shape_frames, name='frames_input')\n",
        "    x = Conv3D(32, (3,3,3), activation='relu', padding='same')(frames_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling3D((1,2,2))(x)\n",
        "    x = Conv3D(64, (3,3,3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling3D((1,2,2))(x)\n",
        "    x = Conv3D(128, (3,3,3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling3D((1,2,2))(x)\n",
        "    x = Conv3D(128, (3,3,3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling3D((1,2,2))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "    x = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = LSTM(32, kernel_regularizer=l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Keypoints input branch\n",
        "    keypoints_input = Input(shape=input_shape_keypoints, name='keypoints_input')\n",
        "    y = TimeDistributed(Flatten())(keypoints_input)\n",
        "    y = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "    y = LSTM(32, kernel_regularizer=l2(0.01))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "    y = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(y)\n",
        "    y = Dropout(0.5)(y)\n",
        "\n",
        "    # Combine both branches\n",
        "    combined = concatenate([x, y])\n",
        "    z = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(combined)\n",
        "    z = Dropout(0.5)(z)\n",
        "    z = Dense(num_classes, activation='sigmoid')(z)\n",
        "\n",
        "    model = Model(inputs=[frames_input, keypoints_input], outputs=z)\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "input_shape_frames = (MAX_NUM_FRAMES, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "input_shape_keypoints = (MAX_NUM_FRAMES, BODY_KEYPOINTS, 3)\n",
        "video_model = create_video_model(input_shape_frames=input_shape_frames, input_shape_keypoints=input_shape_keypoints, num_classes=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z29nangkKR5V",
      "metadata": {
        "collapsed": true,
        "id": "z29nangkKR5V"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, TimeDistributed, LSTM\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy import stats as st\n",
        "import progressbar\n",
        "\n",
        "m = create_video_model()\n",
        "m.summary()\n",
        "\n",
        "def plot_model_diagram(model, file_path='model_diagram.png'):\n",
        "    plot_model(model, to_file=file_path, show_shapes=True, show_layer_names=True)\n",
        "    img = plt.imread(file_path)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "plot_model_diagram(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cO4xm9EdhTD7",
      "metadata": {
        "id": "cO4xm9EdhTD7"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o6egcu-fhY7B",
      "metadata": {
        "id": "o6egcu-fhY7B"
      },
      "source": [
        "## Offline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612bc4941b179af7",
      "metadata": {
        "id": "612bc4941b179af7"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Train the model\n",
        "model = create_video_model()\n",
        "if LOAD_MODEL:\n",
        "    trained_model, train_history, val_generator = load_model(MODELS_PATH, 'basic_model.keras')\n",
        "else:\n",
        "    trained_model, train_history, val_generator = train_model_using_generators(model, epochs=EPOCHS)\n",
        "\n",
        "model.save(os.path.join(MODELS_PATH, 'basic_model.keras'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m4YWSfiayhf6",
      "metadata": {
        "id": "m4YWSfiayhf6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('train_history.pkl', 'wb') as f:\n",
        "    pickle.dump(train_history.history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FT3poEHR9uvR",
      "metadata": {
        "id": "FT3poEHR9uvR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n",
        "\n",
        "def show_model_metrics(train_history, model, val_generator):\n",
        "    # Access training and validation loss/accuracy\n",
        "    train_loss = train_history.history[\"loss\"]\n",
        "    val_loss = train_history.history[\"val_loss\"]\n",
        "    train_acc = train_history.history[\"accuracy\"]\n",
        "    val_acc = train_history.history[\"val_accuracy\"]\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_loss, label=\"Training Loss\")\n",
        "    plt.plot(val_loss, label=\"Validation Loss\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_acc, label=\"Training Accuracy\")\n",
        "    plt.plot(val_acc, label=\"Validation Accuracy\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate model on validation data\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch_x, batch_y in val_generator:\n",
        "        y_true.extend(batch_y)\n",
        "        y_pred_batch = model.predict(batch_x)\n",
        "        y_pred.extend(np.round(y_pred_batch).astype(int))\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Ensure both y_true and y_pred are binary and have the same shape\n",
        "    if y_true.ndim > 1:\n",
        "        y_true = y_true.flatten()\n",
        "    if y_pred.ndim > 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # Trim y_pred to match the length of y_true if needed\n",
        "    if len(y_pred) > len(y_true):\n",
        "        y_pred = y_pred[:len(y_true)]\n",
        "    elif len(y_true) > len(y_pred):\n",
        "        y_true = y_true[:len(y_pred)]\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print('y_true', y_true);\n",
        "    print('y_pred', y_pred);\n",
        "    print(classification_report(y_true, y_pred, target_names=['Normal', 'Violent']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Violent'])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "    average_precision = average_precision_score(y_true, y_pred)\n",
        "\n",
        "    # Plot Precision-Recall curve\n",
        "    plt.figure()\n",
        "    plt.step(recall, precision, where='post', label='Precision-Recall curve (AP = %0.2f)' % average_precision)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.title('Precision-Recall curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_model_metrics(train_history, trained_model, val_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d509bc67e91f8a",
      "metadata": {
        "collapsed": false,
        "id": "58d509bc67e91f8a"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}